{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary Libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.stats as st\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score #sklearn.metrics.classification\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from itertools import cycle\n",
    "from math import exp\n",
    "from math import log\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Model, load_model\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers import Input, Dropout, Add, Dense \n",
    "from keras.layers import MaxPooling1D, Reshape, Activation\n",
    "from keras.layers import BatchNormalization, Flatten, Conv1D\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataPreparation_DNN(data_input_file):\n",
    "  data = np.load(data_input_file, allow_pickle=True)\n",
    "  X = data['X']\n",
    "  X = X[:, 0, :, :]\n",
    "  Y = data['y']\n",
    "  n_class = Y.shape[1]\n",
    "  folds = data['folds']\n",
    "  return X,Y,folds,n_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Models\n",
    "\n",
    "def build_model_FUSEDLSTMCNN_1(row,col,num_classes): \n",
    "  layers = [\n",
    "      \n",
    "      tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten()),\n",
    "      \n",
    "      tf.keras.layers.LSTM(28,return_sequences=True,  input_shape=(row, col)),  # 28\n",
    "      tf.keras.layers.LSTM(28,return_sequences=True),                           # 28\n",
    "      tf.keras.layers.LSTM(28,return_sequences=True),                           # 28\n",
    "      tf.keras.layers.Dropout(0.1),\n",
    "      tf.keras.layers.Conv1D(filters=64,kernel_size=5,strides=2,activation='relu'),\n",
    "      tf.keras.layers.MaxPool1D(pool_size=2,strides = 2),\n",
    "      tf.keras.layers.Conv1D(filters=128,kernel_size=3,strides=2,activation='relu'),\n",
    "      tf.keras.layers.GlobalAveragePooling1D(),        \n",
    "      tf.keras.layers.BatchNormalization(),     \n",
    "        \n",
    "      tf.keras.layers.Dense(num_classes, activation = 'softmax')\n",
    "       ]\n",
    "      \n",
    "  model = tf.keras.Sequential(layers)\n",
    "  return model\n",
    "\n",
    "\n",
    "def build_FUSEDLSTMCNN2_DNN(row,col,num_classes):\n",
    "\n",
    "  tf.keras.initializers.GlorotNormal(234)\n",
    "  input_layer = tf.keras.Input(shape=(row,col,1,))\n",
    "  \n",
    "  layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())(input_layer)\n",
    "  layer = tf.keras.layers.LSTM(28,return_sequences=True)(layer)\n",
    "  layer = tf.keras.layers.Dropout(0.1)(layer)\n",
    "  layer = tf.keras.layers.LSTM(28,return_sequences=True)(layer)\n",
    "  layer = tf.keras.layers.Dropout(0.1)(layer)\n",
    "  layerconv1 = tf.keras.layers.Conv1D(filters=30,kernel_size=5,  strides=1) (layer)\n",
    "  layerconv1 = tf.keras.layers.Activation('relu')(layerconv1)\n",
    "  layerconv2 = tf.keras.layers.Conv1D(filters=40,kernel_size= 10,strides=1) (layer)\n",
    "  layerconv2 = tf.keras.layers.Activation('relu')(layerconv2)\n",
    "  layerconv3 = tf.keras.layers.Conv1D(filters=50,kernel_size= 15,strides=1) (layer)\n",
    "  layerconv3 = tf.keras.layers.Activation('relu')(layerconv3)\n",
    "  layerconv4 = tf.keras.layers.Conv1D(filters=60,kernel_size= 20,strides=1) (layer)\n",
    "  layerconv4 = tf.keras.layers.Activation('relu')(layerconv4)\n",
    "  max1= tf.reduce_max(layerconv1, 1)\n",
    "  max2= tf.reduce_max(layerconv2, 1)\n",
    "  max3= tf.reduce_max(layerconv3, 1)\n",
    "  max4= tf.reduce_max(layerconv4, 1)\n",
    "  concat_layer = tf.keras.layers.concatenate([max1,max2,max3,max4],1)\n",
    "  layer= tf.keras.layers.Dense(num_classes, activation = 'softmax')   (concat_layer) \n",
    "\n",
    "  model = tf.keras.Model(inputs=input_layer, outputs=layer)\n",
    "  return model\n",
    "\n",
    "\n",
    "def identity_block(input, kernel_size, filters, stage, block):\n",
    "  # Variables\n",
    "  filters1, filters2, filters3 = filters\n",
    "  conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "  bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "  # Create layers\n",
    "  output = keras.layers.Conv1D(filters1, 1, kernel_initializer='he_normal', name=conv_name_base + '2a')(input)\n",
    "  output = keras.layers.BatchNormalization(name=bn_name_base + '2a')(output)\n",
    "  output = keras.layers.Activation('relu')(output)\n",
    "  output = keras.layers.Conv1D(filters2, kernel_size, padding='same', kernel_initializer='he_normal', name=conv_name_base + '2b')(output)\n",
    "  output = keras.layers.BatchNormalization(name=bn_name_base + '2b')(output)\n",
    "  output = keras.layers.Activation('relu')(output)\n",
    "  output = keras.layers.Conv1D(filters3, 1, kernel_initializer='he_normal', name=conv_name_base + '2c')(output)\n",
    "  output = keras.layers.BatchNormalization(name=bn_name_base + '2c')(output)\n",
    "  output = keras.layers.add([output, input])\n",
    "  output = keras.layers.Activation('relu')(output)\n",
    "  # Return a block\n",
    "  return output\n",
    "\n",
    "\n",
    "def conv_block(input, kernel_size, filters, stage, block, strides=2):\n",
    "  # Variables\n",
    "  filters1, filters2, filters3 = filters\n",
    "  conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "  bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "  # Create block layers\n",
    "  output = keras.layers.Conv1D(filters1, 1, strides=strides, kernel_initializer='he_normal', name=conv_name_base + '2a')(input)\n",
    "  output = keras.layers.BatchNormalization(name=bn_name_base + '2a')(output)\n",
    "  output = keras.layers.Activation('relu')(output)\n",
    "  output = keras.layers.Conv1D(filters2, kernel_size, padding='same', kernel_initializer='he_normal', name=conv_name_base + '2b')(output)\n",
    "  output = keras.layers.BatchNormalization(name=bn_name_base + '2b')(output)\n",
    "  output = keras.layers.Activation('relu')(output)\n",
    "  output = keras.layers.Conv1D(filters3, 1, kernel_initializer='he_normal', name=conv_name_base + '2c')(output)\n",
    "  output = keras.layers.BatchNormalization(name=bn_name_base + '2c')(output)\n",
    "  shortcut = keras.layers.Conv1D(filters3, 1, strides=strides, kernel_initializer='he_normal', name=conv_name_base + '1')(input)\n",
    "  shortcut = keras.layers.BatchNormalization(name=bn_name_base + '1')(shortcut)\n",
    "  output = keras.layers.add([output, shortcut])\n",
    "  output = keras.layers.Activation('relu')(output)\n",
    "  # Return a block\n",
    "  return output\n",
    "\n",
    "\n",
    "\n",
    "def build_model_ResNet(row, col, num_classes):\n",
    "  input_shape = (row,col)\n",
    "  # Create an input layer \n",
    "  input = keras.layers.Input(shape=input_shape)\n",
    "  # Create output layers\n",
    "  output = keras.layers.Conv1D(64, 7, strides=2, use_bias=False, name='conv1')(input)\n",
    "  output = keras.layers.BatchNormalization(name='bn_conv1')(output)\n",
    "  output = keras.layers.Activation('relu', name='conv1_relu')(output)\n",
    "  output = keras.layers.MaxPooling1D(3, strides=2, padding='same', name='pool1')(output)\n",
    "  output = conv_block(output, 3, [64, 64, 256], stage=2, block='a', strides=1)\n",
    "  output = identity_block(output, 3, [64, 64, 256], stage=2, block='b')\n",
    "  output = identity_block(output, 3, [64, 64, 256], stage=2, block='c')\n",
    "  output = conv_block(output, 3, [128, 128, 512], stage=3, block='a')\n",
    "  output = identity_block(output, 3, [128, 128, 512], stage=3, block='b')\n",
    "  output = identity_block(output, 3, [128, 128, 512], stage=3, block='c')\n",
    "  output = identity_block(output, 3, [128, 128, 512], stage=3, block='d')\n",
    "  output = conv_block(output, 3, [256, 256, 1024], stage=4, block='a')\n",
    "  output = identity_block(output, 3, [256, 256, 1024], stage=4, block='b')\n",
    "  output = identity_block(output, 3, [256, 256, 1024], stage=4, block='c')\n",
    "  output = identity_block(output, 3, [256, 256, 1024], stage=4, block='d')\n",
    "  output = identity_block(output, 3, [256, 256, 1024], stage=4, block='e')\n",
    "  output = identity_block(output, 3, [256, 256, 1024], stage=4, block='f')\n",
    "  output = conv_block(output, 3, [512, 512, 2048], stage=5, block='a')\n",
    "  output = identity_block(output, 3, [512, 512, 2048], stage=5, block='b')\n",
    "  output = identity_block(output, 3, [512, 512, 2048], stage=5, block='c')\n",
    "  output = keras.layers.GlobalAveragePooling1D(name='pool5')(output)\n",
    "  output = keras.layers.Dense(num_classes, activation='softmax', name='fc1000')(output)\n",
    "  # Create a model from input layer and output layers\n",
    "  model = keras.models.Model(inputs=input, outputs=output)\n",
    "  ## Print model\n",
    "  # print(model.summary(), '\\n')\n",
    "  return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_FUSEDLSTMCNN_1(X, y, X_train, X_test, y_train, y_test, n_class):\n",
    "  _,img_rows, img_cols = X.shape\n",
    "  X=X.reshape(X.shape[0],img_rows,img_cols,1)\n",
    "  _,img_rows, img_cols,_ = X.shape\n",
    "\n",
    "  model=build_model_FUSEDLSTMCNN_1(img_rows,img_cols,n_class)\n",
    "  \n",
    "  model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='ADAM')\n",
    "  model.fit(X_train, y_train, batch_size=192, epochs=250,verbose=2)\n",
    "  y_pred = model.predict(X_test)\n",
    "  # if (i==2):\n",
    "  confidence_factor_list = []\n",
    "  for i in range(0, y_pred.shape[0]):\n",
    "    total_fuzzy_sum = sum(y_pred[i])\n",
    "    confidence_factor_activity = y_pred[i]/total_fuzzy_sum\n",
    "    confidence_factor_list.append(confidence_factor_activity)\n",
    "  confidence_factor_array = np.asarray(confidence_factor_list)\n",
    "\n",
    "  return confidence_factor_array\n",
    "\n",
    "\n",
    "def Train_FUSEDLSTMCNN_2(X, y, X_train, X_test, y_train, y_test, n_class):\n",
    "  _, img_rows, img_cols = X.shape\n",
    "  X=X.reshape(X.shape[0],img_rows,img_cols,1)\n",
    "  _,img_rows, img_cols,_ = X.shape\n",
    "    \n",
    "  model=build_FUSEDLSTMCNN2_DNN(img_rows, img_cols, n_class)\n",
    "  optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "  model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer,  metrics=['accuracy'])\n",
    "  model.fit(X_train, y_train, batch_size=batch_size,epochs=200,verbose=2)\n",
    "  y_pred = model.predict(X_test)\n",
    "  confidence_factor_list = []\n",
    "  for i in range(0, y_pred.shape[0]):\n",
    "    total_fuzzy_sum = sum(y_pred[i])\n",
    "    confidence_factor_activity = y_pred[i]/total_fuzzy_sum\n",
    "    confidence_factor_list.append(confidence_factor_activity)\n",
    "  confidence_factor_array = np.asarray(confidence_factor_list)\n",
    "\n",
    "  return confidence_factor_array\n",
    "\n",
    "\n",
    "def Train(X_Train, Y_Train, X_Test, Y_Test, clf):\n",
    "  confidence_factor_list = []\n",
    "  clf.fit(X_Train, Y_Train)\n",
    "  f1 = clf.predict_proba(X_Test)\n",
    "\n",
    "  for i in range(0, f1.shape[0]):\n",
    "    total_fuzzy_sum = sum(f1[i])\n",
    "    confidence_factor_activity = f1[i]/total_fuzzy_sum\n",
    "    # print(confidence_factor_activity)\n",
    "    confidence_factor_list.append(confidence_factor_activity)\n",
    "  confidence_factor_array = np.asarray(confidence_factor_list) #, dtype=np.float32)\n",
    "  # print(\"Confidence_factor_array shape: \", confidence_factor_array.shape)\n",
    "\n",
    "  return confidence_factor_array, Y_Test\n",
    "\n",
    "\n",
    "def Train_ResNet(X, y, X_train, X_test, y_train, y_test, n_class):\n",
    "  _,img_rows, img_cols = X.shape\n",
    "  X=X.reshape(X.shape[0],img_rows,img_cols,1)\n",
    "  _,img_rows, img_cols,_ = X.shape\n",
    "\n",
    "  model=build_model_ResNet(img_rows,img_cols,n_class)\n",
    "  \n",
    "  model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='ADAM')\n",
    "  model.fit(X_train, y_train, batch_size=192, epochs=150,verbose=2)\n",
    "  y_pred = model.predict(X_test)\n",
    "  confidence_factor_list = []\n",
    "  for i in range(0, y_pred.shape[0]):\n",
    "    total_fuzzy_sum = sum(y_pred[i])\n",
    "    confidence_factor_activity = y_pred[i]/total_fuzzy_sum\n",
    "    confidence_factor_list.append(confidence_factor_activity)\n",
    "  confidence_factor_array = np.asarray(confidence_factor_list)\n",
    "\n",
    "  return confidence_factor_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "from math import exp\n",
    "from math import log\n",
    "\n",
    "def mitcherlich(time, alpha, beta, rate):\n",
    "    \"\"\"\n",
    "    time : time\n",
    "    alpha : upper asymptote\n",
    "    beta : growth range\n",
    "    rate : growth rate\n",
    "    \"\"\"\n",
    "    result = alpha - beta * rate ** time\n",
    "    return result\n",
    "\n",
    "\n",
    "def blumberg(time, alpha, slope, w0=1):\n",
    "    \"\"\"\n",
    "    time : time\n",
    "    alpha : upper asymptote\n",
    "    w0 : a reference value at time = time_0\n",
    "    slope : slope of growth\n",
    "    \"\"\"\n",
    "    result = (alpha * (time) ** slope) / (w0 + (time) ** slope)\n",
    "    return result\n",
    "    # result -- 1 (later -1 thing do)\n",
    "    # result -- 0 (later -1 thing)\n",
    "\n",
    "\n",
    "def weibull(time, alpha, beta, rate, slope):\n",
    "    \"\"\"\n",
    "    time : time\n",
    "    alpha : upper asymptote\n",
    "    beta : growth displacement\n",
    "    rate : growth rate\n",
    "    slope : slope of growth\n",
    "    \"\"\"\n",
    "    result = alpha - beta * exp(-rate * time ** slope)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateFuzzyRank_Mitcherlich(confidence_factor_array):  \n",
    "  fuzzy_rank_list = []\n",
    "  for item in confidence_factor_array:\n",
    "    arr=[]\n",
    "    for confidence_factor in item:\n",
    "      fuzzy_rank = mitcherlich(confidence_factor,2,1,2)\n",
    "      arr.append(fuzzy_rank)\n",
    "    fuzzy_rank_list.append(arr)\n",
    "\n",
    "  return np.asarray(fuzzy_rank_list)\n",
    "\n",
    "def GenerateFuzzyRank_Blumberg(confidence_factor_array):  \n",
    "  fuzzy_rank_list = []\n",
    "  for item in confidence_factor_array:\n",
    "    arr=[]\n",
    "    for confidence_factor in item:\n",
    "      fuzzy_rank = 1 - blumberg(confidence_factor, 1, 0.0001, w0=0.5)\n",
    "      arr.append(fuzzy_rank)\n",
    "    fuzzy_rank_list.append(arr)\n",
    "\n",
    "  return np.asarray(fuzzy_rank_list)\n",
    "\n",
    "def GenerateFuzzyRank_Weibull(confidence_factor_array):  \n",
    "  fuzzy_rank_list = []\n",
    "  for item in confidence_factor_array:\n",
    "    arr=[]\n",
    "    for confidence_factor in item:\n",
    "      fuzzy_rank = -weibull(confidence_factor, 0, 0.5, 2, 2) # We negate this \n",
    "      arr.append(fuzzy_rank)\n",
    "    fuzzy_rank_list.append(arr)\n",
    "\n",
    "  return np.asarray(fuzzy_rank_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRFS(fuzzy_rank_array_1, fuzzy_rank_array_2, fuzzy_rank_array_3):\n",
    "    RFS = []\n",
    "    for j in range(len(fuzzy_rank_array_1)):\n",
    "        arr = []\n",
    "        temp_fuzzy_rank_array_1 = fuzzy_rank_array_1[j]\n",
    "        temp_fuzzy_rank_array_2 = fuzzy_rank_array_2[j]\n",
    "        temp_fuzzy_rank_array_3 = fuzzy_rank_array_3[j]\n",
    "        for i in range(len(temp_fuzzy_rank_array_1)):\n",
    "            arr.append(min(temp_fuzzy_rank_array_1[i], temp_fuzzy_rank_array_2[i], temp_fuzzy_rank_array_3[i]))\n",
    "        RFS.append(arr)\n",
    "    return np.asarray(RFS)\n",
    "\n",
    "def getGR(RFS_1, RFS_2, RFS_3):\n",
    "    GR = []\n",
    "    for j in range(len(RFS_1)):\n",
    "        arr = []\n",
    "        temp_RFS_1 = RFS_1[j]\n",
    "        temp_RFS_2 = RFS_2[j]\n",
    "        temp_RFS_3 = RFS_3[j]\n",
    "        for i in range(len(temp_RFS_1)):\n",
    "            arr.append(temp_RFS_1[i]+temp_RFS_2[i]+temp_RFS_3[i])\n",
    "        GR.append(arr)\n",
    "    return np.asarray(GR)\n",
    "\n",
    "def getGC(CFS_1, CFS_2, CFS_3):\n",
    "    GC = []\n",
    "    for j in range(len(CFS_1)):\n",
    "        arr = []\n",
    "        temp_CFS_1 = CFS_1[j]\n",
    "        temp_CFS_2 = CFS_2[j]\n",
    "        temp_CFS_3 = CFS_3[j]    \n",
    "        for i in range(len(temp_CFS_1)):\n",
    "            temp = (1-temp_CFS_1[i]) + (1-temp_CFS_2[i]) + (1-temp_CFS_3[i])\n",
    "            arr.append(temp)\n",
    "        GC.append(arr)\n",
    "    return np.asarray(GC)\n",
    "\n",
    "def getRF(CFS_1, CFS_2, CFS_3, S):\n",
    "    RF = []\n",
    "    for j in range(len(CFS_1)):  \n",
    "\n",
    "        temp_CFS_1 = CFS_1[j]\n",
    "        temp_CFS_2 = CFS_2[j]\n",
    "        temp_CFS_3 = CFS_3[j]\n",
    "\n",
    "        Store_1 = []\n",
    "        Rank_1 = [0]*len(temp_CFS_1)\n",
    "        for i in range(len(temp_CFS_1)):\n",
    "            Store_1.append([i, temp_CFS_1[i]])\n",
    "        Store_1 = sorted(Store_1, reverse=True, key=lambda x:x[1])\n",
    "        for i in range(len(Rank_1)):\n",
    "            for j in range(len(Store_1)):\n",
    "                if i == Store_1[j][0]:\n",
    "                    Rank_1[i] = j+1\n",
    "                    break\n",
    "    \n",
    "        Store_2 = []\n",
    "        Rank_2 = [0]*len(temp_CFS_2)\n",
    "        for i in range(len(temp_CFS_2)):\n",
    "            Store_2.append([i, temp_CFS_2[i]])\n",
    "        Store_2 = sorted(Store_2, reverse=True, key=lambda x:x[1])\n",
    "        for i in range(len(Rank_2)):\n",
    "            for j in range(len(Store_2)):\n",
    "                if i == Store_2[j][0]:\n",
    "                    Rank_2[i] = j+1\n",
    "                    break\n",
    "\n",
    "        Store_3 = []\n",
    "        Rank_3 = [0]*len(temp_CFS_3)\n",
    "        for i in range(len(temp_CFS_3)):\n",
    "            Store_3.append([i, temp_CFS_3[i]])\n",
    "        Store_3 = sorted(Store_3, reverse=True, key=lambda x:x[1])\n",
    "        for i in range(len(Rank_3)):\n",
    "            for j in range(len(Store_3)):\n",
    "                if i == Store_3[j][0]:\n",
    "                    Rank_3[i] = j+1\n",
    "                    break\n",
    "\n",
    "        arr = []\n",
    "        for i in range(len(temp_CFS_1)):\n",
    "            temp = 1/((1 - ((Rank_1[i])/(S+1))) + (1 - ((Rank_2[i])/(S+1))) + (1 - ((Rank_3[i])/(S+1))))\n",
    "            arr.append(temp)\n",
    "\n",
    "        RF.append(arr)\n",
    "\n",
    "    return np.asarray(RF)\n",
    "\n",
    "\n",
    "def PredictClassFinal(GR, GC, RF):\n",
    "    Y_Pred = []\n",
    "    Y_Pred_Score = []\n",
    "\n",
    "    for j in range(len(GR)): \n",
    "        temp_dict = dict()\n",
    "        temp_GR = GR[j]\n",
    "        temp_GC = GC[j]\n",
    "        temp_RF = RF[j]   \n",
    "        for i in range(len(temp_GR)):\n",
    "            temp_dict[i] = temp_GR[i]*temp_GC[i]*temp_RF[i]\n",
    "        ans = min(temp_dict, key=temp_dict.get)\n",
    "        Y_Pred.append(ans)\n",
    "\n",
    "        temp_y_pred_score = []\n",
    "        for i in range(len(temp_dict)):\n",
    "            score = 1/temp_dict[i]\n",
    "            temp_y_pred_score.append(score)\n",
    "\n",
    "        x_array = np.array(temp_y_pred_score)\n",
    "        normalized_arr = preprocessing.normalize([x_array])\n",
    "        Y_Pred_Score.append(np.array(normalized_arr[0])) \n",
    "    return np.asarray(Y_Pred), np.asarray(Y_Pred_Score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for testing -- Uncomment and Test!!!!\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "  X,y,folds,n_class = DataPreparation_DNN('/content/drive/MyDrive/SNOW/USCHAD.npz')\n",
    "\n",
    "  avg_acc_list = []\n",
    "  avg_recall_list = []\n",
    "  avg_f1_list = []\n",
    "\n",
    "  for i in range(0, len(folds)):\n",
    "    train_idx = folds[i][0]\n",
    "    test_idx = folds[i][1]\n",
    "    X_train = X[train_idx]\n",
    "    X_test = X[test_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_test = y[test_idx]\n",
    "\n",
    "    # For classifier 1\n",
    "    confidence_factor_array_1 = Train_FUSEDLSTMCNN_1(X, y, X_train, X_test, y_train, y_test, n_class)\n",
    "    fuzzy_rank_array_1 = GenerateFuzzyRank_Mitcherlich(confidence_factor_array_1)\n",
    "    fuzzy_rank_array_2 = GenerateFuzzyRank_Blumberg(confidence_factor_array_1)\n",
    "    fuzzy_rank_array_3 = GenerateFuzzyRank_Weibull(confidence_factor_array_1)\n",
    "\n",
    "    RFS_1 = getRFS(fuzzy_rank_array_1, fuzzy_rank_array_2, fuzzy_rank_array_3)\n",
    "\n",
    "    \n",
    "    # For classifier 2\n",
    "    confidence_factor_array_2 = Train_FUSEDLSTMCNN_2(X, y, X_train, X_test, y_train, y_test, n_class)\n",
    "    fuzzy_rank_array_4 = GenerateFuzzyRank_Mitcherlich(confidence_factor_array_2)\n",
    "    fuzzy_rank_array_5 = GenerateFuzzyRank_Blumberg(confidence_factor_array_2)\n",
    "    fuzzy_rank_array_6 = GenerateFuzzyRank_Weibull(confidence_factor_array_2)\n",
    "\n",
    "    RFS_2 = getRFS(fuzzy_rank_array_4, fuzzy_rank_array_5, fuzzy_rank_array_6)\n",
    "\n",
    "\n",
    "    \n",
    "    # For classifier 3\n",
    "    confidence_factor_array_3 = Train_ResNet(X, y, X_train, X_test, y_train, y_test, n_class)\n",
    "    fuzzy_rank_array_7 = GenerateFuzzyRank_Mitcherlich(confidence_factor_array_3)\n",
    "    fuzzy_rank_array_8 = GenerateFuzzyRank_Blumberg(confidence_factor_array_3)\n",
    "    fuzzy_rank_array_9 = GenerateFuzzyRank_Weibull(confidence_factor_array_3)\n",
    "\n",
    "    RFS_3 = getRFS(fuzzy_rank_array_7, fuzzy_rank_array_8, fuzzy_rank_array_9)\n",
    "\n",
    "\n",
    "    GR = getGR(RFS_1, RFS_2, RFS_3)\n",
    "    GC = getGC(confidence_factor_array_1, confidence_factor_array_2, confidence_factor_array_3)\n",
    "    RF = getRF(confidence_factor_array_1, confidence_factor_array_2, confidence_factor_array_3, n_class)\n",
    "\n",
    "    y_pred, y_score = PredictClassFinal(GR, GC, RF)\n",
    "\n",
    "    # Correct format of y_test (becomes 1D)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    \n",
    "    acc_fold = accuracy_score(y_test, y_pred)\n",
    "    avg_acc_list.append(acc_fold)\n",
    "    recall_fold = recall_score(y_test, y_pred, average='macro')\n",
    "    avg_recall_list.append(recall_fold)\n",
    "    f1_fold  = f1_score(y_test, y_pred, average='macro')\n",
    "    avg_f1_list.append(f1_fold)\n",
    "\n",
    "    print('Accuracy[{:.4f}] Recall[{:.4f}] F1[{:.4f}] at Fold[{}]'.format(acc_fold, recall_fold, f1_fold ,i+1))\n",
    "    print('________________________________________________________________')\n",
    "\n",
    "  avg_acc = np.asarray(avg_acc_list)\n",
    "  avg_recall = np.asarray(avg_recall_list)\n",
    "  avg_f1 = np.asarray(avg_f1_list)\n",
    "  print(\"\\n\")\n",
    "  print('Overall Accuracy[{:.4f}] Overall Recall[{:.4f}] Overall F1[{:.4f}]'.format(np.mean(avg_acc), np.mean(avg_recall), np.mean(avg_f1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
