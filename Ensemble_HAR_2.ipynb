{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble HAR - 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "meShuJDoJpx5"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzDdcxIBgyse"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqLd8RHBg0Ml"
      },
      "source": [
        "# Import necessary Libraries\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy.stats as st\n",
        "import math\n",
        "from sklearn import preprocessing\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics.classification import accuracy_score, recall_score, f1_score\n",
        "from math import exp\n",
        "from math import log\n",
        "import tensorflow as tf\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "# from keras.utils import to_categorical\n",
        "from keras.models import Model, load_model\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.layers import Input, Dropout, Add, Dense \n",
        "from keras.layers import MaxPooling1D, Reshape, Activation\n",
        "from keras.layers import BatchNormalization, Flatten, Conv1D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8iWJmU3g8G5"
      },
      "source": [
        "def DataPreparation(data_input_file):\n",
        "  data = np.load(data_input_file, allow_pickle=True)\n",
        "  # for k in data.files:\n",
        "  #   print(k)\n",
        "  X = data['X']\n",
        "  # print(X.shape)\n",
        "  X = X[:, 0, :, :]\n",
        "  # print(X.shape)\n",
        "  Y = data['y']\n",
        "  # print(Y.shape)\n",
        "  folds = data['folds']\n",
        "  classes_number = Y.shape[1]\n",
        "  Y = np.argmax(Y, axis=1)\n",
        "  return X,Y,folds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjT6-B74GOzr"
      },
      "source": [
        "def DataPreparation_DNN(data_input_file):\n",
        "  data = np.load(data_input_file, allow_pickle=True)\n",
        "  X = data['X']\n",
        "  X = X[:, 0, :, :]\n",
        "  Y = data['y']\n",
        "  n_class = Y.shape[1]\n",
        "  folds = data['folds']\n",
        " \n",
        "  return X,Y,folds,n_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO7NxA8YhB4B"
      },
      "source": [
        "# State-of-the-art ML models\n",
        "def DTC():\n",
        "  from sklearn.tree import DecisionTreeClassifier\n",
        "  model1 = DecisionTreeClassifier()\n",
        "  return model1\n",
        "\n",
        "def KNC():\n",
        "  from sklearn.neighbors import KNeighborsClassifier\n",
        "  model2 = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 3)\n",
        "  return model2\n",
        "\n",
        "def LRC():\n",
        "  from sklearn.linear_model import LogisticRegression\n",
        "  model3 = LogisticRegression(multi_class='ovr')\n",
        "  return model3\n",
        "\n",
        "def RForest():\n",
        "  from sklearn.ensemble import RandomForestClassifier\n",
        "  model4 = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "  return model4\n",
        "\n",
        "def AdaBoost():\n",
        "  from sklearn.ensemble import AdaBoostClassifier\n",
        "  model5 = AdaBoostClassifier(n_estimators=150, random_state=0)\n",
        "  return model5\n",
        "\n",
        "def RBFC():\n",
        "  model6 = SVC(C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0,\n",
        "      shrinking=True, probability=True, tol=0.001, cache_size=200,\n",
        "      class_weight=None, verbose=False, max_iter=- 1, decision_function_shape='ovr',\n",
        "      break_ties=False, random_state=None)\n",
        "  return model6\n",
        "\n",
        "\n",
        "# Deep Learning Models\n",
        "def build_Lyu_DNN(row,col,num_classes):\n",
        "  tf.keras.initializers.GlorotNormal(234)\n",
        "  input_layer = tf.keras.Input(shape=(row,col,1,))\n",
        "  layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())(input_layer)\n",
        "  layer = tf.keras.layers.LSTM(28,return_sequences=True)(layer)\n",
        "  layer = tf.keras.layers.Dropout(0.1)(layer)\n",
        "  layer = tf.keras.layers.LSTM(28,return_sequences=True)(layer)\n",
        "  layer = tf.keras.layers.Dropout(0.1)(layer)\n",
        "  layerconv1 = tf.keras.layers.Conv1D(filters=30,kernel_size=5,  strides=1) (layer)\n",
        "  layerconv1 = tf.keras.layers.Activation('relu')(layerconv1)\n",
        "  layerconv2 = tf.keras.layers.Conv1D(filters=40,kernel_size= 10,strides=1) (layer)\n",
        "  layerconv2 = tf.keras.layers.Activation('relu')(layerconv2)\n",
        "  layerconv3 = tf.keras.layers.Conv1D(filters=50,kernel_size= 15,strides=1) (layer)\n",
        "  layerconv3 = tf.keras.layers.Activation('relu')(layerconv3)\n",
        "  layerconv4 = tf.keras.layers.Conv1D(filters=60,kernel_size= 20,strides=1) (layer)\n",
        "  layerconv4 = tf.keras.layers.Activation('relu')(layerconv4)\n",
        "  max1= tf.reduce_max(layerconv1, 1)\n",
        "  max2= tf.reduce_max(layerconv2, 1)\n",
        "  max3= tf.reduce_max(layerconv3, 1)\n",
        "  max4= tf.reduce_max(layerconv4, 1)\n",
        "  concat_layer = tf.keras.layers.concatenate([max1,max2,max3,max4],1)\n",
        "  layer= tf.keras.layers.Dense(num_classes, activation = 'softmax')   (concat_layer) \n",
        "\n",
        "  model = tf.keras.Model(inputs=input_layer, outputs=layer)\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "def build_model_JGH(row,col,num_classes): \n",
        "  layers = [\n",
        "      tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten()),\n",
        "      tf.keras.layers.LSTM(28,return_sequences=True,  input_shape=(row, col)),  \n",
        "      tf.keras.layers.LSTM(28,return_sequences=True),                           \n",
        "      tf.keras.layers.LSTM(28,return_sequences=True),                           \n",
        "      tf.keras.layers.Dropout(0.1),\n",
        "      tf.keras.layers.Conv1D(filters=64,kernel_size=5,strides=2,activation='relu'),\n",
        "      tf.keras.layers.MaxPool1D(pool_size=2,strides = 2),\n",
        "      tf.keras.layers.Conv1D(filters=128,kernel_size=3,strides=2,activation='relu'),\n",
        "      tf.keras.layers.GlobalAveragePooling1D(),        \n",
        "      tf.keras.layers.BatchNormalization(),     \n",
        "      tf.keras.layers.Dense(num_classes, activation = 'softmax')\n",
        "       ]\n",
        "      \n",
        "  model = tf.keras.Sequential(layers)\n",
        "  return model\n",
        "\n",
        "\n",
        "def identity_block(input, kernel_size, filters, stage, block):\n",
        "  # Variables\n",
        "  filters1, filters2, filters3 = filters\n",
        "  conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "  bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "  # Create layers\n",
        "  output = keras.layers.Conv1D(filters1, 1, kernel_initializer='he_normal', name=conv_name_base + '2a')(input)\n",
        "  output = keras.layers.BatchNormalization(name=bn_name_base + '2a')(output)\n",
        "  output = keras.layers.Activation('relu')(output)\n",
        "  output = keras.layers.Conv1D(filters2, kernel_size, padding='same', kernel_initializer='he_normal', name=conv_name_base + '2b')(output)\n",
        "  output = keras.layers.BatchNormalization(name=bn_name_base + '2b')(output)\n",
        "  output = keras.layers.Activation('relu')(output)\n",
        "  output = keras.layers.Conv1D(filters3, 1, kernel_initializer='he_normal', name=conv_name_base + '2c')(output)\n",
        "  output = keras.layers.BatchNormalization(name=bn_name_base + '2c')(output)\n",
        "  output = keras.layers.add([output, input])\n",
        "  output = keras.layers.Activation('relu')(output)\n",
        "  # Return a block\n",
        "  return output\n",
        "\n",
        "\n",
        "def conv_block(input, kernel_size, filters, stage, block, strides=2):\n",
        "  # Variables\n",
        "  filters1, filters2, filters3 = filters\n",
        "  conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "  bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "  # Create block layers\n",
        "  output = keras.layers.Conv1D(filters1, 1, strides=strides, kernel_initializer='he_normal', name=conv_name_base + '2a')(input)\n",
        "  output = keras.layers.BatchNormalization(name=bn_name_base + '2a')(output)\n",
        "  output = keras.layers.Activation('relu')(output)\n",
        "  output = keras.layers.Conv1D(filters2, kernel_size, padding='same', kernel_initializer='he_normal', name=conv_name_base + '2b')(output)\n",
        "  output = keras.layers.BatchNormalization(name=bn_name_base + '2b')(output)\n",
        "  output = keras.layers.Activation('relu')(output)\n",
        "  output = keras.layers.Conv1D(filters3, 1, kernel_initializer='he_normal', name=conv_name_base + '2c')(output)\n",
        "  output = keras.layers.BatchNormalization(name=bn_name_base + '2c')(output)\n",
        "  shortcut = keras.layers.Conv1D(filters3, 1, strides=strides, kernel_initializer='he_normal', name=conv_name_base + '1')(input)\n",
        "  shortcut = keras.layers.BatchNormalization(name=bn_name_base + '1')(shortcut)\n",
        "  output = keras.layers.add([output, shortcut])\n",
        "  output = keras.layers.Activation('relu')(output)\n",
        "  # Return a block\n",
        "  return output\n",
        "  \n",
        "def build_model_ResNet(row, col, num_classes):\n",
        "  input_shape = (row,col)\n",
        "  # Create an input layer \n",
        "  input = keras.layers.Input(shape=input_shape)\n",
        "  # Create output layers\n",
        "  output = keras.layers.ZeroPadding1D(padding=2, name='padding_conv1')(input)                   #############\n",
        "  output = keras.layers.Conv1D(64, 7, strides=2, use_bias=False, name='conv1')(output)          #############\n",
        "  output = keras.layers.BatchNormalization(name='bn_conv1')(output)\n",
        "  output = keras.layers.Activation('relu', name='conv1_relu')(output)\n",
        "  output = keras.layers.MaxPooling1D(3, strides=2, padding='same', name='pool1')(output)\n",
        "  output = conv_block(output, 3, [64, 64, 256], stage=2, block='a', strides=1)\n",
        "  output = identity_block(output, 3, [64, 64, 256], stage=2, block='b')\n",
        "  output = identity_block(output, 3, [64, 64, 256], stage=2, block='c')\n",
        "  output = conv_block(output, 3, [128, 128, 512], stage=3, block='a')\n",
        "  output = identity_block(output, 3, [128, 128, 512], stage=3, block='b')\n",
        "  output = identity_block(output, 3, [128, 128, 512], stage=3, block='c')\n",
        "  output = identity_block(output, 3, [128, 128, 512], stage=3, block='d')\n",
        "  output = conv_block(output, 3, [256, 256, 1024], stage=4, block='a')\n",
        "  output = identity_block(output, 3, [256, 256, 1024], stage=4, block='b')\n",
        "  output = identity_block(output, 3, [256, 256, 1024], stage=4, block='c')\n",
        "  output = identity_block(output, 3, [256, 256, 1024], stage=4, block='d')\n",
        "  output = identity_block(output, 3, [256, 256, 1024], stage=4, block='e')\n",
        "  output = identity_block(output, 3, [256, 256, 1024], stage=4, block='f')\n",
        "  output = conv_block(output, 3, [512, 512, 2048], stage=5, block='a')\n",
        "  output = identity_block(output, 3, [512, 512, 2048], stage=5, block='b')\n",
        "  output = identity_block(output, 3, [512, 512, 2048], stage=5, block='c')\n",
        "  output = keras.layers.GlobalAveragePooling1D(name='pool5')(output)\n",
        "  output = keras.layers.Dense(num_classes, activation='softmax', name='fc1000')(output)\n",
        "  # Create a model from input layer and output layers\n",
        "  model = keras.models.Model(inputs=input, outputs=output)\n",
        "  # # Print model\n",
        "  # print()\n",
        "  # print(model.summary(), '\\n')\n",
        "  # Compile the model\n",
        "  # Return a model\n",
        "  return model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6syyoxgnWz7-"
      },
      "source": [
        "# 15 features that we are extracting\n",
        "def A(sample):\n",
        "  feat = []\n",
        "  for col in range(0,sample.shape[1]):\n",
        "      average = np.average(sample[:,col])\n",
        "      feat.append(average)\n",
        "\n",
        "  return feat\n",
        "\n",
        "def SD(sample):\n",
        "  feat = []\n",
        "  for col in range(0, sample.shape[1]):\n",
        "      std = np.std(sample[:, col])\n",
        "      feat.append(std)\n",
        "\n",
        "  return feat\n",
        "\n",
        "def AAD(sample):\n",
        "  feat = []\n",
        "  for col in range(0, sample.shape[1]):\n",
        "      data = sample[:, col]\n",
        "      add = np.mean(np.absolute(data - np.mean(data)))\n",
        "      feat.append(add)\n",
        "\n",
        "  return feat\n",
        "\n",
        "def ARA(sample):\n",
        "  #Average Resultant Acceleration[1]:\n",
        "  # Average of the square roots of the sum of the values of each axis squared √(xi^2 + yi^2+ zi^2) over the ED\n",
        "  feat = []\n",
        "  sum_square = 0\n",
        "  sample = np.power(sample, 2)\n",
        "  for col in range(0, sample.shape[1]):\n",
        "      sum_square = sum_square + sample[:, col]\n",
        "\n",
        "  sample = np.sqrt(sum_square)\n",
        "  average = np.average(sample)\n",
        "  feat.append(average)\n",
        "  return feat\n",
        "\n",
        "def TBP(sample):\n",
        "  from scipy import signal\n",
        "  feat = []\n",
        "  sum_of_time = 0\n",
        "  for col in range(0, sample.shape[1]):\n",
        "      data = sample[:, col]\n",
        "      peaks = signal.find_peaks_cwt(data, np.arange(1,4))\n",
        "      feat.append(peaks.tolist())\n",
        "  return feat\n",
        "\n",
        "def mean_ts(sample, Te=1.0):\n",
        "  feat = []\n",
        "  for col in range(0,sample.shape[1]):\n",
        "    average = np.mean(sample[:,col])\n",
        "    feat.append(average)\n",
        "\n",
        "  return feat\n",
        "\n",
        "def max_ts(sample, Te=1.0):\n",
        "  feat = []\n",
        "  for col in range(0,sample.shape[1]):\n",
        "      average = np.amax(sample[:,col])\n",
        "      feat.append(average)\n",
        "\n",
        "  return feat\n",
        "\n",
        "def min_ts(sample, Te=1.0): \n",
        "  feat = []\n",
        "  for col in range(0,sample.shape[1]):\n",
        "      average = np.amin(sample[:,col]) # min\n",
        "      feat.append(average)\n",
        "\n",
        "  return feat \n",
        "\n",
        "def skew_ts(sample, Te=1.0):    \n",
        "  feat = []\n",
        "  for col in range(0,sample.shape[1]):\n",
        "      average = st.skew(sample[:,col]) # skew\n",
        "      feat.append(average)\n",
        "\n",
        "  return feat\n",
        "\n",
        "def kurtosis_ts(sample, Te=1.0):  \n",
        "  feat = []\n",
        "  for col in range(0,sample.shape[1]):\n",
        "      average = st.kurtosis(sample[:,col]) # kurtosis \n",
        "      feat.append(average)\n",
        "\n",
        "  return feat\n",
        " \n",
        "def iqr_ts(sample, Te=1.0):  \n",
        "  feat = []\n",
        "  for col in range(0,sample.shape[1]):\n",
        "      average = st.iqr(sample[:,col]) # interquartile rate\n",
        "      feat.append(average)\n",
        "\n",
        "  return feat\n",
        "\n",
        "def mad_ts(sample, Te=1.0):  \n",
        "  feat = []\n",
        "  for col in range(0,sample.shape[1]):\n",
        "      average = np.median(np.sort(abs(sample[:,col] - np.median(sample[:,col]))))\n",
        "      feat.append(average)\n",
        "\n",
        "  return feat\n",
        "\n",
        "def area_ts(sample, Te=1.0):  \n",
        "  feat = []\n",
        "  for col in range(0,sample.shape[1]):\n",
        "      average = np.trapz(sample[:,col], dx=Te)\n",
        "      feat.append(average)\n",
        "\n",
        "  return feat                            \n",
        " \n",
        "\n",
        "def sq_area_ts(sample, Te=1.0):\n",
        "  feat = []\n",
        "  for col in range(0,sample.shape[1]):\n",
        "      average = np.trapz(sample[:,col] ** 2, dx=Te)\n",
        "      feat.append(average)\n",
        "\n",
        "  return feat\n",
        "def energy_measure(sample):\n",
        "  \"\"\"Calculates energy measures\"\"\"\n",
        "  feat = []\n",
        "  for col in range(0,sample.shape[1]):\n",
        "\n",
        "    em_x = np.mean(np.square(sample[:,col] ))\n",
        "    feat.append(em_x)\n",
        "  return feat\n",
        "\n",
        "def stat_area_features(x, Te=1.0):\n",
        "  mean_ts = np.mean(x, axis=1).reshape(-1,1) # mean\n",
        "  max_ts = np.amax(x, axis=1).reshape(-1,1) # max\n",
        "  min_ts = np.amin(x, axis=1).reshape(-1,1) # min\n",
        "  std_ts = np.std(x, axis=1).reshape(-1,1) # std\n",
        "  skew_ts = st.skew(x, axis=1).reshape(-1,1) # skew\n",
        "  kurtosis_ts = st.kurtosis(x, axis=1).reshape(-1,1) # kurtosis \n",
        "  iqr_ts = st.iqr(x, axis=1).reshape(-1,1) # interquartile rante\n",
        "  mad_ts = np.median(np.sort(abs(x - np.median(x, axis=1).reshape(-1,1)),\n",
        "                              axis=1), axis=1).reshape(-1,1) # median absolute deviation\n",
        "  area_ts = np.trapz(x, axis=1, dx=Te).reshape(-1,1) # area under curve\n",
        "  sq_area_ts = np.trapz(x ** 2, axis=1, dx=Te).reshape(-1,1) # area under curve ** 2\n",
        "\n",
        "  return np.concatenate((mean_ts,max_ts,min_ts,std_ts,skew_ts,kurtosis_ts,\n",
        "                          iqr_ts,mad_ts,area_ts,sq_area_ts), axis=1)\n",
        "  \n",
        "\n",
        "def frequency_domain_features(x, Te=1.0):\n",
        "  feat=[]\n",
        "  # As the DFT coefficients and their corresponding frequencies are symetrical arrays\n",
        "  # with respect to the middle of the array we need to know if the number of readings \n",
        "  # in x is even or odd to then split the arrays...\n",
        "  if x.shape[1]%2 == 0:\n",
        "      N = int(x.shape[1]/2)\n",
        "  else:\n",
        "      N = int(x.shape[1]/2) - 1\n",
        "  xf = np.repeat(fftfreq(x.shape[1],d=Te)[:N].reshape(1,-1), x.shape[0], axis=0) # frequencies\n",
        "  dft = np.abs(fft(x, axis=1))[:,:N] # DFT coefficients   \n",
        "  \n",
        "  # statistical and area features\n",
        "  # dft_features = stat_area_features(dft, Te=1.0)\n",
        "  # weighted mean frequency\n",
        "  dft_weighted_mean_f = np.average(xf, axis=1, weights=dft).reshape(-1,1)\n",
        "  # 5 first DFT coefficients \n",
        "  dft_first_coef = dft[:,:5]    \n",
        "  # 5 first local maxima of DFT coefficients and their corresponding frequencies\n",
        "  dft_max_coef = np.zeros((x.shape[0],5))\n",
        "  dft_max_coef_f = np.zeros((x.shape[0],5))\n",
        "  for row in range(x.shape[0]):\n",
        "      # finds all local maximas indexes\n",
        "      extrema_ind = argrelextrema(dft[row,:], np.greater, axis=0) \n",
        "      # makes a list of tuples (DFT_i, f_i) of all the local maxima\n",
        "      # and keeps the 5 biggest...\n",
        "      extrema_row = sorted([(dft[row,:][j],xf[row,j]) for j in extrema_ind[0]],\n",
        "                            key=operator.itemgetter(0), reverse=True)[:5] \n",
        "      for i, ext in enumerate(extrema_row):\n",
        "          dft_max_coef[row,i] = ext[0]\n",
        "          dft_max_coef_f[row,i] = ext[1]  \n",
        "\n",
        "  feat.append(np.concatenate((dft_weighted_mean_f,dft_first_coef, dft_max_coef,dft_max_coef_f), axis=1).tolist() )\n",
        "  # print(len(feat))\n",
        "  # print(feat)\n",
        "  return feat\n",
        "\n",
        "## Feature Extraction\n",
        "\n",
        "def feature_extraction(X):\n",
        "  # Extracts the features, as mentioned by Catal et al. 2015\n",
        "  # Average - A,\n",
        "  # Standard Deviation - SD,\n",
        "  # Average Absolute Difference - AAD,\n",
        "  # Average Resultant Acceleration - ARA(1),\n",
        "  # Time Between Peaks - TBP\n",
        "  X_tmp = []\n",
        "  \n",
        "  for sample in X:\n",
        "      features = A(sample)\n",
        "      features = np.hstack((features, A(sample)))\n",
        "      features = np.hstack((features, SD(sample)))\n",
        "      features = np.hstack((features, AAD(sample)))\n",
        "      features = np.hstack((features, ARA(sample)))\n",
        "      \"\"\"\n",
        "      features = np.hstack((features, mean_ts(sample)))\n",
        "      features = np.hstack((features, max_ts(sample)))\n",
        "      features = np.hstack((features, min_ts(sample)))\n",
        "      features = np.hstack((features, kurtosis_ts(sample)))\n",
        "      features = np.hstack((features, iqr_ts(sample)))\n",
        "      features = np.hstack((features, skew_ts(sample)))\n",
        "      features = np.hstack((features, area_ts(sample)))\n",
        "      features = np.hstack((features, sq_area_ts(sample)))\n",
        "      features = np.hstack((features, mad_ts(sample)))\n",
        "      features = np.hstack((features, mean_ts(sample[:,1:]-sample[:,:-1],1)))\n",
        "      features = np.hstack((features, max_ts(sample[:,1:]-sample[:,:-1],1)))\n",
        "      features = np.hstack((features, min_ts(sample[:,1:]-sample[:,:-1],1)))\n",
        "      features = np.hstack((features, kurtosis_ts(sample[:,1:]-sample[:,:-1],1)))\n",
        "      features = np.hstack((features, iqr_ts(sample[:,1:]-sample[:,:-1],1)))\n",
        "      features = np.hstack((features, skew_ts(sample[:,1:]-sample[:,:-1],1)))\n",
        "      features = np.hstack((features, area_ts(sample[:,1:]-sample[:,:-1],1)))\n",
        "      features = np.hstack((features, sq_area_ts(sample[:,1:]-sample[:,:-1],1)))\n",
        "      features = np.hstack((features, mad_ts(sample[:,1:]-sample[:,:-1],1)))\n",
        "      \"\"\"\n",
        "      X_tmp.append(features)\n",
        "      \n",
        "  X = np.array(X_tmp)\n",
        "  \n",
        "  return X\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZukDAfL3HN9C"
      },
      "source": [
        "batch_size = 128\n",
        "epochs = 100\n",
        "learning_rate = 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t98jR5cYPhWP"
      },
      "source": [
        "# #For JSun\n",
        "# def hidden_layer_generate(cnn_model,X):\n",
        "\n",
        "#     \"\"\"\n",
        "#     CNNの中間層の出力を取得するモデルの構築\n",
        "#     :param cnn_model: CNNモデル\n",
        "#     :return:\n",
        "#     \"\"\"\n",
        "\n",
        "#     layer_name = 'flatten_1'\n",
        "#     hidden_layer_model = tf.keras.models.Model(inputs=cnn_model.input, outputs=cnn_model.layers[6].output)\n",
        "\n",
        "#     cnn_train_result = hidden_layer_model.predict(X)\n",
        "\n",
        "#     return hidden_layer_model, cnn_train_result\n",
        "# def elm_model_generate(hidden_layer_cnn_lstm, y,x,num_classes):\n",
        "\n",
        "#     \"\"\"\n",
        "#     ELMモデルの構築\n",
        "#     \"\"\"\n",
        "\n",
        "#     target_train_oh = tf.keras.utils.to_categorical(y, num_classes)\n",
        "#     print(target_train_oh.shape)\n",
        "\n",
        "#     elm_model = hpelm.elm.ELM(hidden_layer_cnn_lstm.shape[1], num_classes)\n",
        "#     print(hidden_layer_cnn_lstm.shape)\n",
        "#     elm_model.add_neurons(128, func='sigm')\n",
        "\n",
        "#     elm_model.train(hidden_layer_cnn_lstm, y, 'c')\n",
        "\n",
        "#     return elm_model\n",
        "\n",
        "# def predict_ELM(cnn_part, elm_part, X):\n",
        "\n",
        "  \n",
        "\n",
        "#   cnn_result = cnn_part.predict(X)\n",
        "#   elm_result = elm_part.predict(cnn_result)\n",
        "#   return elm_result    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x12q_wOJ-nH"
      },
      "source": [
        "def Train(X_Train, Y_Train, X_Test, Y_Test, clf):\n",
        "  confidence_factor_list = []\n",
        "  clf.fit(X_Train, Y_Train)\n",
        "  f1 = clf.predict_proba(X_Test)\n",
        "  for i in range(0, f1.shape[0]):\n",
        "    total_fuzzy_sum = sum(f1[i])\n",
        "    confidence_factor_activity = f1[i]/total_fuzzy_sum\n",
        "    confidence_factor_list.append(confidence_factor_activity)\n",
        "  confidence_factor_array = np.asarray(confidence_factor_list)\n",
        "  return confidence_factor_array, Y_Test\n",
        "\n",
        "\n",
        "def Train_Lyu(X, y, X_train, X_test, y_train, y_test, n_class):\n",
        "  _, img_rows, img_cols = X.shape\n",
        "  X=X.reshape(X.shape[0],img_rows,img_cols,1)\n",
        "  _,img_rows, img_cols,_ = X.shape\n",
        "  model=build_Lyu_DNN(img_rows, img_cols, n_class)\n",
        "  optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer,  metrics=['accuracy'])\n",
        "  model.fit(X_train, y_train, batch_size=batch_size,epochs=200,verbose=2)\n",
        "  y_pred = model.predict(X_test)\n",
        "  confidence_factor_list = []\n",
        "  for i in range(0, y_pred.shape[0]):\n",
        "    total_fuzzy_sum = sum(y_pred[i])\n",
        "    confidence_factor_activity = y_pred[i]/total_fuzzy_sum\n",
        "    confidence_factor_list.append(confidence_factor_activity)\n",
        "  confidence_factor_array = np.asarray(confidence_factor_list)\n",
        "  return confidence_factor_array\n",
        "\n",
        "  \n",
        "def Train_JGH(X, y, X_train, X_test, y_train, y_test, n_class): \n",
        "  _,img_rows, img_cols = X.shape\n",
        "  X=X.reshape(X.shape[0],img_rows,img_cols,1)\n",
        "  _,img_rows, img_cols,_ = X.shape\n",
        "  model=build_model_JGH(img_rows,img_cols,n_class)\n",
        "  model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='ADAM')\n",
        "  model.fit(X_train, y_train, batch_size=192, epochs=250,verbose=2)\n",
        "  y_pred = model.predict(X_test)\n",
        "  confidence_factor_list = []\n",
        "  for i in range(0, y_pred.shape[0]):\n",
        "    total_fuzzy_sum = sum(y_pred[i])\n",
        "    confidence_factor_activity = y_pred[i]/total_fuzzy_sum\n",
        "    confidence_factor_list.append(confidence_factor_activity)\n",
        "  confidence_factor_array = np.asarray(confidence_factor_list)\n",
        "  return confidence_factor_array\n",
        "\n",
        "\n",
        "def Train_ResNet(X, y, X_train, X_test, y_train, y_test, n_class): \n",
        "  _,img_rows, img_cols = X.shape\n",
        "  X=X.reshape(X.shape[0],img_rows,img_cols,1)\n",
        "  _,img_rows, img_cols,_ = X.shape\n",
        "  model=build_model_ResNet(img_rows,img_cols,n_class)\n",
        "  model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='ADAM')\n",
        "  model.fit(X_train, y_train, batch_size=192, epochs=150,verbose=2)\n",
        "  y_pred = model.predict(X_test)\n",
        "  confidence_factor_list = []\n",
        "  for i in range(0, y_pred.shape[0]):\n",
        "    total_fuzzy_sum = sum(y_pred[i])\n",
        "    confidence_factor_activity = y_pred[i]/total_fuzzy_sum\n",
        "    confidence_factor_list.append(confidence_factor_activity)\n",
        "  confidence_factor_array = np.asarray(confidence_factor_list)\n",
        "  return confidence_factor_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azD_a8L7disX"
      },
      "source": [
        "def Train_Lyu_Opp(X, Y, X_train, X_test, y_train, y_test, n_class):\n",
        "  n_class = Y.shape[1]\n",
        "  _, img_rows, img_cols = X_train.shape\n",
        "  X=X.reshape(X.shape[0],img_rows,img_cols,1)\n",
        "  _,img_rows, img_cols,_ = X.shape\n",
        "    \n",
        "  model=build_Lyu_DNN(img_rows, img_cols, n_class)\n",
        "  optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer,  metrics=['accuracy'])\n",
        "  model.fit(X_train, y_train, batch_size=batch_size,epochs=200,verbose=2)\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  confidence_factor_list = []\n",
        "  for i in range(0, y_pred.shape[0]):\n",
        "    total_fuzzy_sum = sum(y_pred[i])\n",
        "    confidence_factor_activity = y_pred[i]/total_fuzzy_sum\n",
        "    confidence_factor_list.append(confidence_factor_activity)\n",
        "  confidence_factor_array = np.asarray(confidence_factor_list)\n",
        "  return confidence_factor_array\n",
        "\n",
        "\n",
        "def Train_JGH_Opp(X, Y, X_train, X_test, y_train, y_test, n_class):\n",
        "  n_class = Y.shape[1]\n",
        "  _, img_rows, img_cols = X.shape\n",
        "  X=X.reshape(X.shape[0],img_rows,img_cols,1)\n",
        "  _,img_rows, img_cols,_ = X.shape\n",
        "\n",
        "  model=build_model_JGH(img_rows,img_cols,n_class)\n",
        "  model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='ADAM')\n",
        "  model.fit(X_train, y_train, batch_size=192, epochs=250,verbose=2)\n",
        "  y_pred = model.predict(X_test)\n",
        "  \n",
        "  confidence_factor_list = []\n",
        "  for i in range(0, y_pred.shape[0]):\n",
        "    total_fuzzy_sum = sum(y_pred[i])\n",
        "    confidence_factor_activity = y_pred[i]/total_fuzzy_sum\n",
        "    confidence_factor_list.append(confidence_factor_activity)\n",
        "  confidence_factor_array = np.asarray(confidence_factor_list)\n",
        "  return confidence_factor_array\n",
        "\n",
        "\n",
        "\n",
        "def Train_ResNet_Opp(X, Y, X_train, X_test, y_train, y_test, n_class):\n",
        "  # n_class = Y.shape[1]\n",
        "  _, img_rows, img_cols = X.shape\n",
        "  X=X.reshape(X.shape[0],img_rows,img_cols,1)\n",
        "  _,img_rows, img_cols,_ = X.shape\n",
        "  model=build_model_ResNet(img_rows,img_cols,n_class)\n",
        "  model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='ADAM')\n",
        "  model.fit(X_train, y_train, batch_size=192, epochs=150,verbose=2)\n",
        "  y_pred = model.predict(X_test)\n",
        "  confidence_factor_list = []\n",
        "  for i in range(0, y_pred.shape[0]):\n",
        "    total_fuzzy_sum = sum(y_pred[i])\n",
        "    confidence_factor_activity = y_pred[i]/total_fuzzy_sum\n",
        "    confidence_factor_list.append(confidence_factor_activity)\n",
        "  confidence_factor_array = np.asarray(confidence_factor_list)\n",
        "  return confidence_factor_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el0xwS6_xA6L"
      },
      "source": [
        "def PredictClass_5(merged_fuzzy_rank_array_1, merged_fuzzy_rank_array_2, merged_fuzzy_rank_array_3, merged_fuzzy_rank_array_4, merged_fuzzy_rank_array_5,\n",
        "                confidence_factor_array_1, confidence_factor_array_2, confidence_factor_array_3, confidence_factor_array_4, confidence_factor_array_5):  \n",
        "\n",
        "  Y_Pred = []\n",
        "  RSc = {}\n",
        "  CFSc = {}\n",
        "  FSc = {}\n",
        "  k = 3#int((fuzzy_rank_array_1.shape[1])/2);\n",
        "  PcR = 0.888\n",
        "  PcCF = 0\n",
        "    \n",
        "  for i in range(0, len(fuzzy_rank_array_1)):\n",
        "    item_1 = merged_fuzzy_rank_array_1[i]\n",
        "    item_2 = merged_fuzzy_rank_array_2[i]\n",
        "    item_3 = merged_fuzzy_rank_array_3[i]\n",
        "    item_4 = merged_fuzzy_rank_array_4[i]\n",
        "    item_5 = merged_fuzzy_rank_array_5[i]\n",
        "    indices_1 = sorted(range(len(item_1)), key=lambda x: item_1[x])[0:k]\n",
        "    indices_2 = sorted(range(len(item_2)), key=lambda x: item_2[x])[0:k]\n",
        "    indices_3 = sorted(range(len(item_3)), key=lambda x: item_3[x])[0:k]\n",
        "    indices_4 = sorted(range(len(item_4)), key=lambda x: item_4[x])[0:k]\n",
        "    indices_5 = sorted(range(len(item_5)), key=lambda x: item_5[x])[0:k]\n",
        "\n",
        "    for j in range(0, len(item_1)):\n",
        "      if j in indices_1:\n",
        "        RSc[j] = item_1[j]\n",
        "        CFSc[j] = 1-((1/5)*confidence_factor_array_1[i][j])\n",
        "      else:\n",
        "        RSc[j] = PcR\n",
        "        CFSc[j] = 1-((1/5)*PcCF)\n",
        "      \n",
        "      if j in indices_2:\n",
        "        RSc[j] += item_2[j]\n",
        "        CFSc[j] += -((1/5)*confidence_factor_array_2[i][j])\n",
        "      else:\n",
        "        RSc[j] += PcR\n",
        "        CFSc[j] += -((1/3)*PcCF)\n",
        "\n",
        "      if j in indices_3:\n",
        "        RSc[j] += item_3[j]\n",
        "        CFSc[j] += -((1/5)*confidence_factor_array_3[i][j])\n",
        "      else:\n",
        "        RSc[j] += PcR\n",
        "        CFSc[j] += -((1/5)*PcCF)\n",
        "\n",
        "      if j in indices_4:\n",
        "        RSc[j] += item_4[j]\n",
        "        CFSc[j] += -((1/5)*confidence_factor_array_4[i][j])\n",
        "      else:\n",
        "        RSc[j] += PcR\n",
        "        CFSc[j] += -((1/5)*PcCF)\n",
        "\n",
        "      if j in indices_5:\n",
        "        RSc[j] += item_5[j]\n",
        "        CFSc[j] += -((1/5)*confidence_factor_array_5[i][j])\n",
        "      else:\n",
        "        RSc[j] += PcR\n",
        "        CFSc[j] += -((1/5)*PcCF)\n",
        "\n",
        "    CFSc_keys = [*CFSc]\n",
        "    # print(\"Keys:\", CFSc_keys)\n",
        "    # for key in CFSc_keys:\n",
        "    #   CFSc[key] = (1 - CFSc[key])\n",
        "    # print(\"RSc: \", RSc)\n",
        "    # print(\"CFSc: \", CFSc)\n",
        "    valid_keys = list(set([*RSc]) & set([*CFSc]))\n",
        "    for key in valid_keys:\n",
        "      FSc[key]= RSc[key]* CFSc[key]\n",
        "    # print(\"FSc: \", FSc)\n",
        "    predicted_class = min(FSc, key=FSc.get)\n",
        "    Y_Pred.append(predicted_class)\n",
        "  return np.asarray(Y_Pred)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_30F_3ShisW"
      },
      "source": [
        "def PredictClass_3(merged_fuzzy_rank_array_1, merged_fuzzy_rank_array_2, merged_fuzzy_rank_array_3,\n",
        "                confidence_factor_array_1, confidence_factor_array_2, confidence_factor_array_3):  \n",
        "\n",
        "  Y_Pred = []\n",
        "  RSc = {}\n",
        "  CFSc = {}\n",
        "  FSc = {}\n",
        "  k = 3#int((fuzzy_rank_array_1.shape[1])/2);\n",
        "  PcR = 0.888\n",
        "  PcCF = 0\n",
        "    \n",
        "  for i in range(0, len(fuzzy_rank_array_1)):\n",
        "    item_1 = merged_fuzzy_rank_array_1[i]\n",
        "    item_2 = merged_fuzzy_rank_array_2[i]\n",
        "    item_3 = merged_fuzzy_rank_array_3[i]\n",
        "    indices_1 = sorted(range(len(item_1)), key=lambda x: item_1[x])[0:k]\n",
        "    indices_2 = sorted(range(len(item_2)), key=lambda x: item_2[x])[0:k]\n",
        "    indices_3 = sorted(range(len(item_3)), key=lambda x: item_3[x])[0:k]\n",
        "\n",
        "    for j in range(0, len(item_1)):\n",
        "      if j in indices_1:\n",
        "        RSc[j] = item_1[j]\n",
        "        CFSc[j] = 1-((1/3)*confidence_factor_array_1[i][j])\n",
        "      else:\n",
        "        RSc[j] = PcR\n",
        "        CFSc[j] = 1-((1/3)*PcCF)\n",
        "      \n",
        "      if j in indices_2:\n",
        "        RSc[j] += item_2[j]\n",
        "        CFSc[j] += -((1/3)*confidence_factor_array_2[i][j])\n",
        "      else:\n",
        "        RSc[j] += PcR\n",
        "        CFSc[j] += -((1/3)*PcCF)\n",
        "\n",
        "      if j in indices_3:\n",
        "        RSc[j] += item_3[j]\n",
        "        CFSc[j] += -((1/3)*confidence_factor_array_3[i][j])\n",
        "      else:\n",
        "        RSc[j] += PcR\n",
        "        CFSc[j] += -((1/3)*PcCF)\n",
        "\n",
        "\n",
        "    CFSc_keys = [*CFSc]\n",
        "    # print(\"Keys:\", CFSc_keys)\n",
        "    # for key in CFSc_keys:\n",
        "    #   CFSc[key] = (1 - CFSc[key])\n",
        "    # print(\"RSc: \", RSc)\n",
        "    # print(\"CFSc: \", CFSc)\n",
        "    valid_keys = list(set([*RSc]) & set([*CFSc]))\n",
        "    for key in valid_keys:\n",
        "      FSc[key]= RSc[key]* CFSc[key]\n",
        "    # print(\"FSc: \", FSc)\n",
        "    predicted_class = min(FSc, key=FSc.get)\n",
        "    Y_Pred.append(predicted_class)\n",
        "  return np.asarray(Y_Pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvGxNeI5KFbl"
      },
      "source": [
        "def PredictClass_2(merged_fuzzy_rank_array_1, merged_fuzzy_rank_array_2, confidence_factor_array_1, confidence_factor_array_2):  \n",
        "\n",
        "  Y_Pred = []\n",
        "  RSc = {}\n",
        "  CFSc = {}\n",
        "  FSc = {}\n",
        "  k = 3#int((fuzzy_rank_array_1.shape[1])/2);\n",
        "  PcR = 0.888\n",
        "  PcCF = 0\n",
        "    \n",
        "  for i in range(0, len(fuzzy_rank_array_1)):\n",
        "    item_1 = merged_fuzzy_rank_array_1[i]\n",
        "    item_2 = merged_fuzzy_rank_array_2[i]\n",
        "    indices_1 = sorted(range(len(item_1)), key=lambda x: item_1[x])[0:k]\n",
        "    indices_2 = sorted(range(len(item_2)), key=lambda x: item_2[x])[0:k]\n",
        "\n",
        "    for j in range(0, len(item_1)):\n",
        "      if j in indices_1:\n",
        "        RSc[j] = item_1[j]\n",
        "        CFSc[j] = 1-((1/2)*confidence_factor_array_1[i][j])\n",
        "      else:\n",
        "        RSc[j] = PcR\n",
        "        CFSc[j] = 1-((1/2)*PcCF)\n",
        "      \n",
        "      if j in indices_2:\n",
        "        RSc[j] += item_2[j]\n",
        "        CFSc[j] += -((1/2)*confidence_factor_array_2[i][j])\n",
        "      else:\n",
        "        RSc[j] += PcR\n",
        "        CFSc[j] += -((1/2)*PcCF)\n",
        "\n",
        "    CFSc_keys = [*CFSc]\n",
        "    # print(\"Keys:\", CFSc_keys)\n",
        "    # for key in CFSc_keys:\n",
        "    #   CFSc[key] = (1 - CFSc[key])\n",
        "    # print(\"RSc: \", RSc)\n",
        "    # print(\"CFSc: \", CFSc)\n",
        "    valid_keys = list(set([*RSc]) & set([*CFSc]))\n",
        "    for key in valid_keys:\n",
        "      FSc[key]= RSc[key]* CFSc[key]\n",
        "    # print(\"FSc: \", FSc)\n",
        "    predicted_class = min(FSc, key=FSc.get)\n",
        "    Y_Pred.append(predicted_class)\n",
        "  return np.asarray(Y_Pred)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-R2SjwPhp-L"
      },
      "source": [
        "# Functions\n",
        "from math import exp\n",
        "from math import log\n",
        "    \n",
        "def mitcherlich(time, alpha, beta, rate):\n",
        "    \"\"\"\n",
        "    time : time\n",
        "    alpha : upper asymptote\n",
        "    beta : growth range\n",
        "    rate : growth rate\n",
        "    \"\"\"\n",
        "    result = alpha - beta * rate ** time\n",
        "    return result\n",
        "\n",
        "def blumberg(time, alpha, slope, w0=1):\n",
        "    \"\"\"\n",
        "    time : time\n",
        "    alpha : upper asymptote\n",
        "    w0 : a reference value at time = time_0\n",
        "    slope : slope of growth\n",
        "    \"\"\"\n",
        "    result = (alpha * (time) ** slope) / (w0 + (time) ** slope)\n",
        "    return result\n",
        "\n",
        "def gompertz(time, alpha, beta, rate):\n",
        "    \"\"\"\n",
        "    time : time\n",
        "    alpha : upper asymptote\n",
        "    beta : growth displacement\n",
        "    rate : growth rate\n",
        "    \"\"\"\n",
        "    result = alpha * exp(-beta * exp(-rate * time))\n",
        "    return result\n",
        "\n",
        "def weibull(time, alpha, beta, rate, slope):\n",
        "    \"\"\"\n",
        "    time : time\n",
        "    alpha : upper asymptote\n",
        "    beta : growth displacement\n",
        "    rate : growth rate\n",
        "    slope : slope of growth\n",
        "    \"\"\"\n",
        "    result = alpha - beta * exp(-rate * time ** slope)\n",
        "    return result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRB976WQhslM"
      },
      "source": [
        "def GenerateFuzzyRank_Gaussian(confidence_factor_array):  \n",
        "  fuzzy_rank_list = []\n",
        "  for item in confidence_factor_array:\n",
        "    arr=[]\n",
        "    for confidence_factor in item:\n",
        "      fuzzy_rank = 1-math.exp(((confidence_factor-1.0)*(confidence_factor-1.0)/(-2.0)))\n",
        "      arr.append(fuzzy_rank)\n",
        "    fuzzy_rank_list.append(arr)\n",
        "  return np.asarray(fuzzy_rank_list)\n",
        "\n",
        "def GenerateFuzzyRank_Mitcherlich(confidence_factor_array):  \n",
        "  fuzzy_rank_list = []\n",
        "  for item in confidence_factor_array:\n",
        "    arr=[]\n",
        "    for confidence_factor in item:\n",
        "      fuzzy_rank = mitcherlich(confidence_factor,2,1,2)\n",
        "      arr.append(fuzzy_rank)\n",
        "    fuzzy_rank_list.append(arr)\n",
        "  return np.asarray(fuzzy_rank_list)\n",
        "\n",
        "def GenerateFuzzyRank_Blumberg(confidence_factor_array):  \n",
        "  fuzzy_rank_list = []\n",
        "  for item in confidence_factor_array:\n",
        "    arr=[]\n",
        "    for confidence_factor in item:\n",
        "      fuzzy_rank = 1 - blumberg(confidence_factor, 1, 0.0001, w0=0.5)\n",
        "      arr.append(fuzzy_rank)\n",
        "    fuzzy_rank_list.append(arr)\n",
        "  return np.asarray(fuzzy_rank_list)\n",
        "\n",
        "def GenerateFuzzyRank_Gompertz(confidence_factor_array):  \n",
        "  fuzzy_rank_list = []\n",
        "  for item in confidence_factor_array:\n",
        "    arr=[]\n",
        "    for confidence_factor in item:\n",
        "      fuzzy_rank = 1 - gompertz(confidence_factor, 1, 2, 3)\n",
        "      arr.append(fuzzy_rank)\n",
        "    fuzzy_rank_list.append(arr)\n",
        "  return np.asarray(fuzzy_rank_list)\n",
        "\n",
        "def GenerateFuzzyRank_Weibull(confidence_factor_array):  \n",
        "  fuzzy_rank_list = []\n",
        "  for item in confidence_factor_array:\n",
        "    arr=[]\n",
        "    for confidence_factor in item:\n",
        "      fuzzy_rank = -weibull(confidence_factor, 0, 0.5, 2, 2) # We negate this \n",
        "      arr.append(fuzzy_rank)\n",
        "    fuzzy_rank_list.append(arr)\n",
        "  return np.asarray(fuzzy_rank_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YLufwxAZwcR"
      },
      "source": [
        "def MergeFuzzyRanks(fuzzy_rank_array_1, fuzzy_rank_array_2, fuzzy_rank_array_3, fuzzy_rank_array_4, fuzzy_rank_array_5, confidence_factor_array):\n",
        "  k = 3\n",
        "  merged_fuzzy_rank_array = []\n",
        "  for i in range(len(fuzzy_rank_array_1)):\n",
        "    combined_fuzzy_scores = []\n",
        "    topKindices = set(sorted(range(len(confidence_factor_array[i])), key=lambda x: confidence_factor_array[i][x])[len(confidence_factor_array)-k-1:-1])\n",
        "    # set_topKindices = set(topKindices)\n",
        "    for j in range(len(fuzzy_rank_array_1[i])):  \n",
        "      if j in topKindices:\n",
        "        fuzzy_score = min(fuzzy_rank_array_1[i][j], fuzzy_rank_array_2[i][j], fuzzy_rank_array_3[i][j], fuzzy_rank_array_4[i][j], fuzzy_rank_array_5[i][j])\n",
        "        # print(\"Fuzzy Score: \", fuzzy_score)\n",
        "      else:\n",
        "        fuzzy_score = max(fuzzy_rank_array_1[i][j], fuzzy_rank_array_2[i][j], fuzzy_rank_array_3[i][j], fuzzy_rank_array_4[i][j], fuzzy_rank_array_5[i][j])\n",
        "        # print(\"Fuzzy Score: \", fuzzy_score)\n",
        "      combined_fuzzy_scores.append(fuzzy_score)\n",
        "    # print(\"Combined Fuzzy: \", combined_fuzzy_scores)\n",
        "    merged_fuzzy_rank_array.append(combined_fuzzy_scores)\n",
        "  return np.asarray(merged_fuzzy_rank_array)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wVBVb8Vpn79"
      },
      "source": [
        "# # For ensembles (except Opportunity): -- Uncomment and Test!!\n",
        "\n",
        "# if __name__ == \"__main__\":  \n",
        "#   X,Y,folds = DataPreparation('/content/drive/MyDrive/data/SNOW/WISDM.npz')\n",
        "#   clf1 = DTC()\n",
        "#   clf2 = KNC()\n",
        "#   clf3 = LRC()\n",
        "#   clf4 = RForest()\n",
        "#   clf5 = AdaBoost()\n",
        "#   clf6 = RBFC()\n",
        "\n",
        "#   avg_acc_list = []\n",
        "#   avg_recall_list = []\n",
        "#   avg_f1_list = []\n",
        "\n",
        "#   for i in range(len(folds)):\n",
        "#     Train_idx = folds[i][0]\n",
        "#     Test_idx = folds[i][1]\n",
        "#     # X_Train_nonnormalized = X[Train_idx]\n",
        "#     X_Train = X[Train_idx]\n",
        "#     Y_Train = Y[Train_idx]\n",
        "#     # X_Test_nonnormalized = X[Test_idx]\n",
        "#     X_Test = X[Test_idx]\n",
        "#     Y_Test = Y[Test_idx]\n",
        "\n",
        "#     X_Train = feature_extraction(X_Train)\n",
        "#     # X_Train = preprocessing.normalize(X_Train_nonnormalized, norm='l2')\n",
        "#     X_Test = feature_extraction(X_Test)\n",
        "#     # X_Test = preprocessing.normalize(X_Test_nonnormalized, norm='l2')\n",
        "\n",
        "#     ## Train_idx = folds[i][0]\n",
        "#     ## Test_idx = folds[i][1]\n",
        "#     ## X_Train = X[Train_idx]\n",
        "#     ## Y_Train = Y[Train_idx]\n",
        "#     ## X_Test = X[Test_idx]\n",
        "#     ## Y_Test = Y[Test_idx]\n",
        "#     ## print(X_Test.shape[0])\n",
        "#     ## X_Train = feature_extraction(X_Train)\n",
        "#     ## X_Test = feature_extraction(X_Test)\n",
        "\n",
        "#     # For classifier 1\n",
        "#     confidence_factor_array_1, Y_Test = Train(X_Train, Y_Train, X_Test, Y_Test, clf1)\n",
        "#     fuzzy_rank_array_1 = GenerateFuzzyRank_Gaussian(confidence_factor_array_1)\n",
        "#     fuzzy_rank_array_2 = GenerateFuzzyRank_Mitcherlich(confidence_factor_array_1)\n",
        "#     fuzzy_rank_array_3 = GenerateFuzzyRank_Blumberg(confidence_factor_array_1)\n",
        "#     fuzzy_rank_array_4 = GenerateFuzzyRank_Gompertz(confidence_factor_array_1)\n",
        "#     fuzzy_rank_array_5 = GenerateFuzzyRank_Weibull(confidence_factor_array_1)\n",
        "\n",
        "#     # print(fuzzy_rank_array_1[0])\n",
        "#     # print(fuzzy_rank_array_2[0])\n",
        "#     # print(fuzzy_rank_array_3[0])\n",
        "#     # print(fuzzy_rank_array_4[0])\n",
        "#     # print(fuzzy_rank_array_5[0])\n",
        "#     # print(confidence_factor_array_1)\n",
        "#     # break\n",
        "\n",
        "#     ######\n",
        "#     # print(\"\\nCLASSIFIER 1\")\n",
        "#     # for row in fuzzy_rank_array_1:\n",
        "#     #   print(\"Gaussian: \",row)\n",
        "#     #   break\n",
        "      \n",
        "#     # for row in fuzzy_rank_array_2:\n",
        "#     #   print(\"Mitcherlich: \",row)\n",
        "#     #   break\n",
        "      \n",
        "#     # for row in fuzzy_rank_array_3:\n",
        "#     #   print(\"Blumberg: \",row)\n",
        "#     #   break\n",
        "      \n",
        "#     # for row in fuzzy_rank_array_4:\n",
        "#     #   print(\"Gompertz: \",row)\n",
        "#     #   break\n",
        "      \n",
        "#     # for row in fuzzy_rank_array_5:\n",
        "#     #   print(\"Weibull: \",row)\n",
        "#     #   break\n",
        "#     ######\n",
        "     \n",
        "    \n",
        "#     # print(\"Gaussian: \", fuzzy_rank_array_1)\n",
        "#     # print(\"Mitcherlich: \", fuzzy_rank_array_2)\n",
        "#     # print(\"Blumberg: \",fuzzy_rank_array_3)\n",
        "#     # print(\"Gompertz: \", fuzzy_rank_array_4)\n",
        "#     # print(\"Richard: \", fuzzy_rank_array_5)\n",
        "#     # print(\"\\n\")\n",
        "\n",
        "\n",
        "#     merged_fuzzy_rank_array_1 = MergeFuzzyRanks(fuzzy_rank_array_1, fuzzy_rank_array_2, fuzzy_rank_array_3, \n",
        "#                                               fuzzy_rank_array_4, fuzzy_rank_array_5, confidence_factor_array_1)\n",
        "#     # print(\"Merged Fuzzy Matrix: \", merged_fuzzy_rank_array_1)\n",
        "\n",
        "  \n",
        "#     # For classifier 2\n",
        "#     confidence_factor_array_2, Y_Test = Train(X_Train, Y_Train, X_Test, Y_Test, clf2)\n",
        "#     fuzzy_rank_array_6 = GenerateFuzzyRank_Gaussian(confidence_factor_array_2)\n",
        "#     fuzzy_rank_array_7 = GenerateFuzzyRank_Mitcherlich(confidence_factor_array_2)\n",
        "#     fuzzy_rank_array_8 = GenerateFuzzyRank_Blumberg(confidence_factor_array_2)\n",
        "#     fuzzy_rank_array_9 = GenerateFuzzyRank_Gompertz(confidence_factor_array_2)\n",
        "#     fuzzy_rank_array_10 = GenerateFuzzyRank_Weibull(confidence_factor_array_2)\n",
        "    \n",
        "#     ######\n",
        "#     # print(\"\\nCLASSIFIER 2\")\n",
        "#     # for row in fuzzy_rank_array_6:\n",
        "#     #   print(\"Gaussian: \",row)\n",
        "#     #   break\n",
        "      \n",
        "#     # for row in fuzzy_rank_array_7:\n",
        "#     #   print(\"Mitcherlich: \",row)\n",
        "#     #   break\n",
        "     \n",
        "#     # for row in fuzzy_rank_array_8:\n",
        "#     #   print(\"Blumberg: \",row)\n",
        "#     #   break\n",
        "      \n",
        "#     # for row in fuzzy_rank_array_9:\n",
        "#     #   print(\"Gompertz: \",row)\n",
        "#     #   break\n",
        "     \n",
        "#     # for row in fuzzy_rank_array_10:\n",
        "#     #   print(\"Weibull: \",row)\n",
        "#     #   break\n",
        "#     ######\n",
        "      \n",
        "\n",
        "#     # print(\"Gaussian: \", fuzzy_rank_array_6)\n",
        "#     # print(\"Mitcherlich: \", fuzzy_rank_array_7)\n",
        "#     # print(\"Blumberg: \",fuzzy_rank_array_8)\n",
        "#     # print(\"Gompertz: \", fuzzy_rank_array_9)\n",
        "#     # print(\"Richard: \", fuzzy_rank_array_10)\n",
        "#     # print(\"\\n\")\n",
        "\n",
        "#     merged_fuzzy_rank_array_2 = MergeFuzzyRanks(fuzzy_rank_array_6, fuzzy_rank_array_7, fuzzy_rank_array_8, \n",
        "#                                               fuzzy_rank_array_9, fuzzy_rank_array_10, confidence_factor_array_2)\n",
        "#     # print(\"Merged Fuzzy Matrix: \", merged_fuzzy_rank_array_2)\n",
        "\n",
        "\n",
        "    \n",
        "#     # For classifier 3\n",
        "#     confidence_factor_array_3, Y_Test = Train(X_Train, Y_Train, X_Test, Y_Test, clf6)\n",
        "#     fuzzy_rank_array_11 = GenerateFuzzyRank_Gaussian(confidence_factor_array_3)\n",
        "#     fuzzy_rank_array_12 = GenerateFuzzyRank_Mitcherlich(confidence_factor_array_3)\n",
        "#     fuzzy_rank_array_13 = GenerateFuzzyRank_Blumberg(confidence_factor_array_3)\n",
        "#     fuzzy_rank_array_14 = GenerateFuzzyRank_Gompertz(confidence_factor_array_3)\n",
        "#     fuzzy_rank_array_15 = GenerateFuzzyRank_Weibull(confidence_factor_array_3)\n",
        "    \n",
        "#     ######\n",
        "#     # print(\"\\nCLASSIFIER 3\")\n",
        "#     # for row in fuzzy_rank_array_11:\n",
        "#     #   print(\"Gaussian: \",row)\n",
        "#     #   break\n",
        "      \n",
        "#     # for row in fuzzy_rank_array_12:\n",
        "#     #   print(\"Mitcherlich: \",row)\n",
        "#     #   break\n",
        "     \n",
        "#     # for row in fuzzy_rank_array_13:\n",
        "#     #   print(\"Blumberg: \",row)\n",
        "#     #   break\n",
        "      \n",
        "#     # for row in fuzzy_rank_array_14:\n",
        "#     #   print(\"Gompertz: \",row)\n",
        "#     #   break\n",
        "      \n",
        "#     # for row in fuzzy_rank_array_15:\n",
        "#     #   print(\"Weibull: \",row)\n",
        "#     #   break\n",
        "#     ######\n",
        "      \n",
        "#     # print(\"Gaussian: \", fuzzy_rank_array_11)\n",
        "#     # print(\"Mitcherlich: \", fuzzy_rank_array_12)\n",
        "#     # print(\"Blumberg: \",fuzzy_rank_array_13)\n",
        "#     # print(\"Gompertz: \", fuzzy_rank_array_14)\n",
        "#     # print(\"Richard: \", fuzzy_rank_array_15)\n",
        "#     # print(\"\\n\")\n",
        "\n",
        "\n",
        "#     merged_fuzzy_rank_array_3 = MergeFuzzyRanks(fuzzy_rank_array_11, fuzzy_rank_array_12, fuzzy_rank_array_13, \n",
        "#                                               fuzzy_rank_array_14, fuzzy_rank_array_15, confidence_factor_array_3)\n",
        "#     # print(\"Merged Fuzzy Matrix: \", merged_fuzzy_rank_array_3)\n",
        "\n",
        "\n",
        "    \n",
        "#     # # For classifier 4\n",
        "#     # confidence_factor_array_4, Y_Test = Train(X_Train, Y_Train, X_Test, Y_Test, clf5)\n",
        "#     # fuzzy_rank_array_4 = GenerateFuzzyRank(confidence_factor_array_4)\n",
        "\n",
        "#     # # For classifier 5\n",
        "#     # confidence_factor_array_5, Y_Test = Train(X_Train, Y_Train, X_Test, Y_Test, clf5)\n",
        "#     # fuzzy_rank_array_5 = GenerateFuzzyRank(confidence_factor_array_5)\n",
        "\n",
        "\n",
        "#     # Ensembling all\n",
        "#     ## For 5 classifiers\n",
        "#     # Y_Pred = PredictClass_5(fuzzy_rank_array_1, fuzzy_rank_array_2, fuzzy_rank_array_3, fuzzy_rank_array_4, fuzzy_rank_array_5, \n",
        "#     #                      confidence_factor_array_1, confidence_factor_array_2, confidence_factor_array_3, confidence_factor_array_4, confidence_factor_array_5)\n",
        "#     ## For 3 classifiers\n",
        "#     Y_Pred = PredictClass_3(merged_fuzzy_rank_array_1, merged_fuzzy_rank_array_2, merged_fuzzy_rank_array_3,\n",
        "#                         confidence_factor_array_1, confidence_factor_array_2, confidence_factor_array_3)\n",
        "\n",
        "\n",
        "#     # Results\n",
        "#     # Correct = 0\n",
        "#     # Wrong = 0\n",
        "\n",
        "#     # for i in range(len(Y_Test)):\n",
        "#     #   if Y_Test[i] == Y_Pred[i]:\n",
        "#     #     Correct += 1\n",
        "#     #   else:\n",
        "#     #     Wrong += 1\n",
        "\n",
        "\n",
        "#     ## Do Comment this break\n",
        "#     # print(\"Actual Class: \", Y_Test[i])\n",
        "#     # break\n",
        "\n",
        "    \n",
        "\n",
        "#     # Uncomment for Test\n",
        "#     acc_fold = accuracy_score(Y_Test, Y_Pred)\n",
        "#     avg_acc_list.append(acc_fold)\n",
        "#     recall_fold = recall_score(Y_Test, Y_Pred, average='macro')\n",
        "#     avg_recall_list.append(recall_fold)\n",
        "#     f1_fold  = f1_score(Y_Test, Y_Pred, average='macro')\n",
        "#     avg_f1_list.append(f1_fold)\n",
        "\n",
        "#     # print(\"*************Correct: \",Correct,\"Wrong: \",Wrong,\"Accuracy: \", metrics.accuracy_score(Y_Test, Y_Pred)*100)\n",
        "#     # avg_acc.append((Correct*100)/(Correct+Wrong))\n",
        "#     print('Accuracy[{:.4f}] Recall[{:.4f}] F1[{:.4f}]'.format(acc_fold, recall_fold, f1_fold ,i+1))\n",
        "#     print('________________________________________________________________')\n",
        "\n",
        "#   avg_acc = np.asarray(avg_acc_list)\n",
        "#   avg_recall = np.asarray(avg_recall_list)\n",
        "#   avg_f1 = np.asarray(avg_f1_list)\n",
        "#   # print(\"\\nOverall Accuracy: \", sum(avg_acc)/len(avg_acc))\n",
        "#   print(\"\\n\")\n",
        "#   print('Overall Accuracy[{:.4f}] Overall Recall[{:.4f}] Overall F1[{:.4f}]'.format(np.mean(avg_acc), np.mean(avg_recall), np.mean(avg_f1)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anTeaeGC9r-M"
      },
      "source": [
        "# # Code for testing single ML classifiers -- Uncomment and Test!!!!\n",
        "\n",
        "# if __name__ == \"__main__\":  \n",
        "#   X,Y,folds = DataPreparation('/content/drive/MyDrive/SNOW/WISDM.npz')\n",
        "#   clf1 = DTC()\n",
        "#   clf2 = KNC()\n",
        "#   clf3 = LRC()\n",
        "#   clf4 = RForest()\n",
        "#   clf5 = AdaBoost()\n",
        "#   clf6 = RBFC()\n",
        "\n",
        "#   avg_acc_list = []\n",
        "#   avg_recall_list = []\n",
        "#   avg_f1_list = []\n",
        "\n",
        "#   clf = RBFC()\n",
        "\n",
        "#   avg_acc_list = []\n",
        "#   avg_recall_list = []\n",
        "#   avg_f1_list = []\n",
        "\n",
        "#   for i in range(len(folds)):\n",
        "#     Train_idx = folds[i][0]\n",
        "#     Test_idx = folds[i][1]\n",
        "#     X_Train_nonnormalized = X[Train_idx]\n",
        "#     Y_Train = Y[Train_idx]\n",
        "#     X_Test_nonnormalized = X[Test_idx]\n",
        "#     Y_Test = Y[Test_idx]\n",
        "\n",
        "#     X_Train_nonnormalized = feature_extraction(X_Train_nonnormalized)\n",
        "#     X_Train = preprocessing.normalize(X_Train_nonnormalized, norm='l1')\n",
        "#     X_Test_nonnormalized = feature_extraction(X_Test_nonnormalized)\n",
        "#     X_Test = preprocessing.normalize(X_Test_nonnormalized, norm='l1')\n",
        "\n",
        "\n",
        "#     clf.fit(X_Train, Y_Train)\n",
        "#     f1 = clf.predict(X_Test)  \n",
        "\n",
        "#     acc_fold = accuracy_score(Y_Test, f1)\n",
        "#     avg_acc_list.append(acc_fold)\n",
        "#     recall_fold = recall_score(Y_Test, f1, average='macro')\n",
        "#     avg_recall_list.append(recall_fold)\n",
        "#     f1_fold  = f1_score(Y_Test, f1, average='macro')\n",
        "#     avg_f1_list.append(f1_fold)\n",
        "\n",
        "#     # print(\"*************Correct: \",Correct,\"Wrong: \",Wrong,\"Accuracy: \", metrics.accuracy_score(Y_Test, Y_Pred)*100)\n",
        "#     # avg_acc.append((Correct*100)/(Correct+Wrong))\n",
        "#     print('Accuracy[{:.4f}] Recall[{:.4f}] F1[{:.4f}]'.format(acc_fold, recall_fold, f1_fold ,i+1))\n",
        "#     print('________________________________________________________________')\n",
        "\n",
        "#   avg_acc = np.asarray(avg_acc_list)\n",
        "#   avg_recall = np.asarray(avg_recall_list)\n",
        "#   avg_f1 = np.asarray(avg_f1_list)\n",
        "#   # print(\"\\nOverall Accuracy: \", sum(avg_acc)/len(avg_acc))\n",
        "#   print(\"\\n\")\n",
        "#   print('Overall Accuracy[{:.4f}] Overall Recall[{:.4f}] Overall F1[{:.4f}]'.format(np.mean(avg_acc), np.mean(avg_recall), np.mean(avg_f1)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI7WnZ95ATBV"
      },
      "source": [
        "# # Code for testing ensembled DNN classifiers -- Uncomment and Test!!!!\n",
        "\n",
        "# if __name__ == \"__main__\":  \n",
        "#   X,y,folds,n_class = DataPreparation_DNN('/content/drive/MyDrive/data/SNOW/WHARF.npz')\n",
        "\n",
        "#   avg_acc_list = []\n",
        "#   avg_recall_list = []\n",
        "#   avg_f1_list = []\n",
        "\n",
        "#   for i in range(0, len(folds)):\n",
        "#     train_idx = folds[i][0]\n",
        "#     test_idx = folds[i][1]\n",
        "#     X_train = X[train_idx]\n",
        "#     X_test = X[test_idx]\n",
        "#     y_train = y[train_idx]\n",
        "#     y_test = y[test_idx]\n",
        "\n",
        "#     # For classifier 1\n",
        "#     confidence_factor_array_1 = Train_JGH(X, y, X_train, X_test, y_train, y_test, n_class)\n",
        "#     fuzzy_rank_array_1 = GenerateFuzzyRank_Gaussian(confidence_factor_array_1)\n",
        "#     fuzzy_rank_array_2 = GenerateFuzzyRank_Mitcherlich(confidence_factor_array_1)\n",
        "#     fuzzy_rank_array_3 = GenerateFuzzyRank_Blumberg(confidence_factor_array_1)\n",
        "#     fuzzy_rank_array_4 = GenerateFuzzyRank_Gompertz(confidence_factor_array_1)\n",
        "#     fuzzy_rank_array_5 = GenerateFuzzyRank_Weibull(confidence_factor_array_1)\n",
        "\n",
        "#     merged_fuzzy_rank_array_1 = MergeFuzzyRanks(fuzzy_rank_array_1, fuzzy_rank_array_2, fuzzy_rank_array_3, \n",
        "#                                               fuzzy_rank_array_4, fuzzy_rank_array_5, confidence_factor_array_1)\n",
        "    \n",
        "\n",
        "\n",
        "#     # For classifier 2\n",
        "#     confidence_factor_array_2 = Train_Lyu(X, y, X_train, X_test, y_train, y_test, n_class)\n",
        "#     fuzzy_rank_array_6 = GenerateFuzzyRank_Gaussian(confidence_factor_array_2)\n",
        "#     fuzzy_rank_array_7 = GenerateFuzzyRank_Mitcherlich(confidence_factor_array_2)\n",
        "#     fuzzy_rank_array_8 = GenerateFuzzyRank_Blumberg(confidence_factor_array_2)\n",
        "#     fuzzy_rank_array_9 = GenerateFuzzyRank_Gompertz(confidence_factor_array_2)\n",
        "#     fuzzy_rank_array_10 = GenerateFuzzyRank_Weibull(confidence_factor_array_2)\n",
        "\n",
        "#     merged_fuzzy_rank_array_2 = MergeFuzzyRanks(fuzzy_rank_array_6, fuzzy_rank_array_7, fuzzy_rank_array_8, \n",
        "#                                               fuzzy_rank_array_9, fuzzy_rank_array_10, confidence_factor_array_2)\n",
        "\n",
        "\n",
        "    \n",
        "#     # For classifier 3\n",
        "#     confidence_factor_array_3 = Train_ResNet(X, y, X_train, X_test, y_train, y_test, n_class)\n",
        "#     fuzzy_rank_array_11 = GenerateFuzzyRank_Gaussian(confidence_factor_array_3)\n",
        "#     fuzzy_rank_array_12 = GenerateFuzzyRank_Mitcherlich(confidence_factor_array_3)\n",
        "#     fuzzy_rank_array_13 = GenerateFuzzyRank_Blumberg(confidence_factor_array_3)\n",
        "#     fuzzy_rank_array_14 = GenerateFuzzyRank_Gompertz(confidence_factor_array_3)\n",
        "#     fuzzy_rank_array_15 = GenerateFuzzyRank_Weibull(confidence_factor_array_3)\n",
        "\n",
        "#     merged_fuzzy_rank_array_3 = MergeFuzzyRanks(fuzzy_rank_array_11, fuzzy_rank_array_12, fuzzy_rank_array_13, \n",
        "#                                               fuzzy_rank_array_14, fuzzy_rank_array_15, confidence_factor_array_3)\n",
        "\n",
        "\n",
        "\n",
        "#     y_pred = PredictClass_3(merged_fuzzy_rank_array_1, merged_fuzzy_rank_array_2, merged_fuzzy_rank_array_3,\n",
        "#                             confidence_factor_array_1, confidence_factor_array_2, confidence_factor_array_3)\n",
        "\n",
        "#     # Correct format of y_test (becomes 1D)\n",
        "#     y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "#     # Uncomment for Test\n",
        "#     acc_fold = accuracy_score(y_test, y_pred)\n",
        "#     avg_acc_list.append(acc_fold)\n",
        "#     recall_fold = recall_score(y_test, y_pred, average='macro')\n",
        "#     avg_recall_list.append(recall_fold)\n",
        "#     f1_fold  = f1_score(y_test, y_pred, average='macro')\n",
        "#     avg_f1_list.append(f1_fold)\n",
        "\n",
        "#     # print(\"*************Correct: \",Correct,\"Wrong: \",Wrong,\"Accuracy: \", metrics.accuracy_score(Y_Test, Y_Pred)*100)\n",
        "#     # avg_acc.append((Correct*100)/(Correct+Wrong))\n",
        "#     print('Accuracy[{:.4f}] Recall[{:.4f}] F1[{:.4f}] at Fold[{}]'.format(acc_fold, recall_fold, f1_fold ,i+1))\n",
        "#     print('________________________________________________________________')\n",
        "\n",
        "#   avg_acc = np.asarray(avg_acc_list)\n",
        "#   avg_recall = np.asarray(avg_recall_list)\n",
        "#   avg_f1 = np.asarray(avg_f1_list)\n",
        "#   # print(\"\\nOverall Accuracy: \", sum(avg_acc)/len(avg_acc))\n",
        "#   print(\"\\n\")\n",
        "#   print('Overall Accuracy[{:.4f}] Overall Recall[{:.4f}] Overall F1[{:.4f}]'.format(np.mean(avg_acc), np.mean(avg_recall), np.mean(avg_f1)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l99UOr__mMtI"
      },
      "source": [
        "# Code for testing Opportunity Dataset -- Uncomment and Test!!\n",
        "\n",
        "def OpportunityDataSetPreparation():\n",
        "  X_train=np.load('/content/drive/MyDrive/data/Opportunity/Opportunity_train_X.npz')['arr_0']\n",
        "  X_test=np.load('/content/drive/MyDrive/data/Opportunity/Opportunity_test_X.npz')['arr_0']\n",
        "  Y_train=np.load('/content/drive/MyDrive/data/Opportunity/Opportunity_train_y.npz')['arr_0']\n",
        "  Y_test=np.load('/content/drive/MyDrive/data/Opportunity/Opportunity_test_Y.npz')['arr_0']\n",
        "  X = np.concatenate((X_train, X_test), axis = 0)\n",
        "  Y = np.concatenate((Y_train, Y_test), axis = 0)\n",
        "  classes_number = Y_train.shape[1]\n",
        "  return X, Y, X_train,Y_train,X_test,Y_test,classes_number\n",
        "\n",
        "\n",
        "X, Y, X_Train, Y_Train, X_Test, Y_Test, n_classes = OpportunityDataSetPreparation()\n",
        "#clf1 = DTC()\n",
        "#clf2 = KNC()\n",
        "#clf3 = LRC()\n",
        "#clf4 = RForest()\n",
        "#clf5 = AdaBoost()\n",
        "#clf6 = RBFC()\n",
        "#X_Train = feature_extraction(X_Train)\n",
        "#X_Train = preprocessing.normalize(X_Train_nonnormalized, norm='l1')\n",
        "#X_Test = feature_extraction(X_Test)\n",
        "#X_Test = preprocessing.normalize(X_Test_nonnormalized, norm='l1')\n",
        "\n",
        "\n",
        "# For classifier 1\n",
        "confidence_factor_array_1 = Train_Lyu_Opp(X, Y, X_Train, X_Test, Y_Train, Y_Test, n_classes)\n",
        "fuzzy_rank_array_1 = GenerateFuzzyRank_Gaussian(confidence_factor_array_1)\n",
        "fuzzy_rank_array_2 = GenerateFuzzyRank_Mitcherlich(confidence_factor_array_1)\n",
        "fuzzy_rank_array_3 = GenerateFuzzyRank_Blumberg(confidence_factor_array_1)\n",
        "fuzzy_rank_array_4 = GenerateFuzzyRank_Gompertz(confidence_factor_array_1)\n",
        "fuzzy_rank_array_5 = GenerateFuzzyRank_Weibull(confidence_factor_array_1)\n",
        "\n",
        "# print(fuzzy_rank_array_1[0])\n",
        "# print(fuzzy_rank_array_2[0])\n",
        "# print(fuzzy_rank_array_3[0])\n",
        "# print(fuzzy_rank_array_4[0])\n",
        "# print(fuzzy_rank_array_5[0])\n",
        "# print(confidence_factor_array_1)\n",
        "# break\n",
        "\n",
        "merged_fuzzy_rank_array_1 = MergeFuzzyRanks(fuzzy_rank_array_1, fuzzy_rank_array_2, fuzzy_rank_array_3, \n",
        "                                              fuzzy_rank_array_4, fuzzy_rank_array_5, confidence_factor_array_1)\n",
        "# print(\"Merged Fuzzy Matrix: \", merged_fuzzy_rank_array_1)\n",
        "\n",
        "  \n",
        "# For classifier 2\n",
        "confidence_factor_array_2 = Train_JGH_Opp(X, Y, X_Train, X_Test, Y_Train, Y_Test, n_classes)\n",
        "fuzzy_rank_array_6 = GenerateFuzzyRank_Gaussian(confidence_factor_array_2)\n",
        "fuzzy_rank_array_7 = GenerateFuzzyRank_Mitcherlich(confidence_factor_array_2)\n",
        "fuzzy_rank_array_8 = GenerateFuzzyRank_Blumberg(confidence_factor_array_2)\n",
        "fuzzy_rank_array_9 = GenerateFuzzyRank_Gompertz(confidence_factor_array_2)\n",
        "fuzzy_rank_array_10 = GenerateFuzzyRank_Weibull(confidence_factor_array_2)\n",
        "    \n",
        "merged_fuzzy_rank_array_2 = MergeFuzzyRanks(fuzzy_rank_array_6, fuzzy_rank_array_7, fuzzy_rank_array_8, \n",
        "                                              fuzzy_rank_array_9, fuzzy_rank_array_10, confidence_factor_array_2)\n",
        "# print(\"Merged Fuzzy Matrix: \", merged_fuzzy_rank_array_2)\n",
        "\n",
        "\n",
        "    \n",
        "# For classifier 3\n",
        "confidence_factor_array_3 = Train_ResNet_Opp(X, Y, X_Train, X_Test, Y_Train, Y_Test, n_classes)\n",
        "fuzzy_rank_array_11 = GenerateFuzzyRank_Gaussian(confidence_factor_array_3)\n",
        "fuzzy_rank_array_12 = GenerateFuzzyRank_Mitcherlich(confidence_factor_array_3)\n",
        "fuzzy_rank_array_13 = GenerateFuzzyRank_Blumberg(confidence_factor_array_3)\n",
        "fuzzy_rank_array_14 = GenerateFuzzyRank_Gompertz(confidence_factor_array_3)\n",
        "fuzzy_rank_array_15 = GenerateFuzzyRank_Weibull(confidence_factor_array_3)\n",
        "    \n",
        "merged_fuzzy_rank_array_3 = MergeFuzzyRanks(fuzzy_rank_array_11, fuzzy_rank_array_12, fuzzy_rank_array_13, \n",
        "                                              fuzzy_rank_array_14, fuzzy_rank_array_15, confidence_factor_array_3)\n",
        "# print(\"Merged Fuzzy Matrix: \", merged_fuzzy_rank_array_3)\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "Y_Pred = PredictClass_3(merged_fuzzy_rank_array_1, merged_fuzzy_rank_array_2, merged_fuzzy_rank_array_3,\n",
        "                        confidence_factor_array_1, confidence_factor_array_2, confidence_factor_array_3)\n",
        "\n",
        "Y_Test = np.argmax(y_test, axis=1)    \n",
        "\n",
        "# Uncomment for Test\n",
        "avg_acc = accuracy_score(Y_Test, Y_Pred)\n",
        "avg_recall = recall_score(Y_Test, Y_Pred, average='macro')\n",
        "avg_f1 = f1_score(Y_Test, Y_Pred, average='macro')\n",
        "\n",
        "print('Overall Accuracy[{:.4f}] Overall Recall[{:.4f}] Overall F1[{:.4f}]'.format(avg_acc, avg_recall, avg_f1))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}