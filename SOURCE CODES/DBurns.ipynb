{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DBurns.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i122Jooe8sX"
      },
      "source": [
        "!nvidis-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw3s9chFe3GW"
      },
      "source": [
        "## Libraries Prequisities\n",
        "!pip install scikit-learn\n",
        "!pip install numpy==1.16.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCXLXcRee7e5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dXtzA5teuoE"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.metrics.classification import accuracy_score, recall_score, f1_score\n",
        "import scipy.stats as st\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# %tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "print(tf.test.gpu_device_name())\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def DataPreparation(data_input_file):\n",
        "  print('David M Burns et al. 2018 {}'.format(data_input_file))\n",
        "  data = np.load(data_input_file, allow_pickle=True)\n",
        "  X = data['X']\n",
        "  X = X[:, 0, :, :]\n",
        "  Y = data['y']\n",
        "  folds = data['folds']\n",
        " \n",
        "  return X,Y,folds\n",
        "\n",
        "def OpportunityDataSetPreparation():\n",
        "  X_train=np.load('/content/drive/MyDrive/data/Opportunity/Opportunity_train_X.npz')['arr_0']\n",
        "  X_test=np.load('/content/drive/MyDrive/data/Opportunity/Opportunity_test_X.npz')['arr_0']\n",
        "  Y_train=np.load('/content/drive/MyDrive/data/Opportunity/Opportunity_train_y.npz')['arr_0']\n",
        "  Y_test=np.load('/content/drive/MyDrive/data/Opportunity/Opportunity_test_Y.npz')['arr_0']\n",
        "  classes_number = Y_train.shape[1]\n",
        "  #Y_train = np.argmax(Y_train, axis=1)\n",
        "  #Y_test = np.argmax(Y_test, axis=1)\n",
        "  return X_train,Y_train,X_test,Y_test,classes_number\n",
        "\n",
        "def ReportAccuracies(avg_acc, avg_recall,avg_f1):\n",
        "  ic_acc = st.t.interval(0.9, len(avg_acc) - 1, loc=np.mean(avg_acc), scale=st.sem(avg_acc))\n",
        "  ic_recall = st.t.interval(0.9, len(avg_recall) - 1, loc=np.mean(avg_recall), scale=st.sem(avg_recall))\n",
        "  ic_f1 = st.t.interval(0.9, len(avg_f1) - 1, loc=np.mean(avg_f1), scale=st.sem(avg_f1))\n",
        "  print('Mean Accuracy[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_acc), ic_acc[0], ic_acc[1]))\n",
        "  print('Mean Recall[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_recall), ic_recall[0], ic_recall[1]))\n",
        "  print('Mean F1[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_f1), ic_f1[0], ic_f1[1]))\n",
        "\n",
        "def Run_David_Burns(data_input_file):\n",
        "  X,Y,folds,= DataPreparation(data_input_file)\n",
        " \n",
        "  avg_acc, avg_recall,avg_f1= Train(X,Y,folds)\n",
        "  ReportAccuracies(avg_acc, avg_recall,avg_f1 )\n",
        "\n",
        "\n",
        "def Run_David_Burns_Opportunity():\n",
        "  X_Train, Y_Train, X_Test, Y_Test, n_classes = OpportunityDataSetPreparation()\n",
        "  Train_Opportunity(X_Train, Y_Train, X_Test, Y_Test, n_classes)\n",
        "\n",
        "from tensorflow.keras.layers import Lambda\n",
        "\n",
        "def fcn(input_shape, num_classes, conv_layers=({'f': 128, 'k': 8, 's': 1}, {'f': 256, 'k': 5, 's': 1}, {'f': 128, 'k': 3, 's': 1}),\n",
        "        dropout=0.3, normalize=True, embedding_size=None):\n",
        "    \"\"\"\n",
        "    Creates fully convolutional neural (FCN) network architecture described in: https://arxiv.org/abs/2001.05517\n",
        "    :param input_shape: tuple (2)\n",
        "        segment shape (width, n_channels)\n",
        "    :param conv_layers: tuple of dicts\n",
        "        describe conv layers with f: filters, k: kernel size, s: stride\n",
        "    :param dropout: float\n",
        "        dropout ratio applied at each layer\n",
        "    :param normalize: bool\n",
        "        apply l2 normalization\n",
        "    :param embedding_size: integer, optional\n",
        "        defines embedding size (number of filters for last CNN layer)\n",
        "    :return: keras model\n",
        "        the fcn model\n",
        "    \"\"\"\n",
        "\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    layer = input_layer\n",
        "\n",
        "    if embedding_size:\n",
        "        conv_layers[-1]['f'] = embedding_size\n",
        "\n",
        "    for lp in conv_layers:\n",
        "        layer = tf.keras.layers.Conv1D(filters=lp['f'], kernel_size=lp['k'], strides=lp['s'], padding='same')(layer)\n",
        "        layer = tf.keras.layers.BatchNormalization()(layer)\n",
        "        layer = tf.keras.layers.Activation('relu')(layer)\n",
        "\n",
        "        if dropout:\n",
        "            layer = tf.keras.layers.Dropout(rate=dropout)(layer)\n",
        "\n",
        "    layer = tf.keras.layers.GlobalAveragePooling1D()(layer)\n",
        "\n",
        "    if normalize:\n",
        "        layer = Lambda(lambda x: tf.keras.backend.l2_normalize(x, axis=1))(layer)\n",
        "    layer= tf.keras.layers.Dense(num_classes, activation = 'softmax')   (layer) \n",
        "\n",
        "    return tf.keras.Model(inputs=input_layer, outputs=layer)\n",
        "\n",
        "def Train(X,y,folds):\n",
        "  avg_acc = []\n",
        "  avg_recall = []\n",
        "  avg_f1 = []\n",
        "  n_class = y.shape[1]\n",
        "  _,img_rows, img_cols = X.shape\n",
        "  for i in range(0, len(folds)):\n",
        "    train_idx = folds[i][0]\n",
        "    test_idx = folds[i][1]\n",
        "    X_train = X[train_idx]\n",
        "    X_test = X[test_idx]\n",
        "    model=fcn((img_rows,img_cols),n_class)\n",
        "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='SGD')\n",
        "    model.fit(X_train, y[train_idx], batch_size=64, epochs=150)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "    y_true = np.argmax(y[test_idx], axis=1)\n",
        "    acc_fold = accuracy_score(y_true, y_pred)\n",
        "    avg_acc.append(acc_fold)\n",
        "    recall_fold = recall_score(y_true, y_pred, average='macro')\n",
        "    avg_recall.append(recall_fold)\n",
        "    f1_fold = f1_score(y_true, y_pred, average='macro')\n",
        "    avg_f1.append(f1_fold)\n",
        "    print('Accuracy[{:.4f}] Recall[{:.4f}] F1[{:.4f}] at fold[{}]'.format(acc_fold, recall_fold, f1_fold ,i+1))\n",
        "    print('________________________________________________________________')\n",
        "    \n",
        "  return avg_acc, avg_recall,avg_f1\n",
        "\n",
        "\n",
        "\n",
        "def Train_Opportunity(X_Train, Y_Train, X_Test, Y_Test, n_class):\n",
        "  # avg_acc = []\n",
        "  # avg_recall = []\n",
        "  # avg_f1 = []\n",
        "  n_class = y.shape[1]\n",
        "  _,img_rows, img_cols = X.shape\n",
        "  # for i in range(0, len(folds)):\n",
        "  # train_idx = folds[i][0]\n",
        "  # test_idx = folds[i][1]\n",
        "  # X_train = X[train_idx]\n",
        "  # X_test = X[test_idx]\n",
        "  model=fcn((img_rows,img_cols),n_class)\n",
        "  model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='SGD')\n",
        "  model.fit(X_Train, Y_Train, batch_size=64, epochs=150)\n",
        "  Y_Pred = model.predict(X_Test)\n",
        "  Y_Pred = np.argmax(Y_Pred, axis=1)\n",
        "  Y_Test = np.argmax(Y_Test, axis=1)\n",
        "  acc_fold = accuracy_score(Y_Test, Y_Pred)\n",
        "  # avg_acc.append(acc_fold)\n",
        "  recall_fold = recall_score(Y_Test, Y_Pred, average='macro')\n",
        "  # avg_recall.append(recall_fold)\n",
        "  f1_fold = f1_score(Y_Test, Y_Pred, average='macro')\n",
        "  # avg_f1.append(f1_fold)\n",
        "  print('Accuracy[{:.4f}] Recall[{:.4f}] F1[{:.4f}] at fold[{}]'.format(acc_fold, recall_fold, f1_fold ,i+1))\n",
        "  print('________________________________________________________________')\n",
        "\n",
        "tf.keras.backend.set_image_data_format('channels_first')\n",
        "\n",
        "# Run_David_Burns('/content/drive/My Drive/SNOW/USCHAD.npz')\n",
        "Run_David_Burns_Opportunity()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}