{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAR-Lyu",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBC6QTlOFWIj"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Kc2ma8iJLj9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "664ca271-f899-44fa-da56-f4a2f7a39285"
      },
      "source": [
        "!pip install scikit-learn\n",
        "!pip install numpy==1.16.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Collecting numpy==1.16.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/57/07c49e1a6d2706fb7336b3fb11dd285c1e96535c80833d7524f002f57086/numpy-1.16.1-cp37-cp37m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 187kB/s \n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.16.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyarrow 3.0.0 has requirement numpy>=1.16.6, but you'll have numpy 1.16.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.2.1 has requirement numpy>=1.17, but you'll have numpy 1.16.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "Successfully installed numpy-1.16.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwsZzbXjktB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18a9c56b-8a6c-4374-84c0-681f713b828c"
      },
      "source": [
        "import numpy as np;\n",
        "import random;\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier;\n",
        "from sklearn.metrics.classification import accuracy_score, recall_score, f1_score;\n",
        "import scipy.stats as st;\n",
        "from scipy.stats import gmean;\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0XJJC1s71aw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc10a49-1faf-4030-8ea0-bfc90dfeb86f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgHaSTG2RPp_"
      },
      "source": [
        "np.random.seed(12227)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeAhRKO3VlV0"
      },
      "source": [
        "def DataPreparation(data_input_file):\n",
        "  print('Lingjuan Lyu  et al. 2017 {}'.format(data_input_file))\n",
        "  data = np.load(data_input_file)\n",
        "  X = data['X']\n",
        "  X = X[:, 0, :, :]\n",
        "  \n",
        "  Y = data['y']\n",
        "  folds = data['folds']\n",
        " \n",
        "  return X,Y,folds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG9MTOKo5Jcs"
      },
      "source": [
        "def RunLingjuan_Lyu(data_input_file):\n",
        "  X,Y,folds= DataPreparation(data_input_file)\n",
        "  avg_acc, avg_recall,avg_f1= Train(X,Y,folds)\n",
        "  ReportAccuracies(avg_acc, avg_recall,avg_f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou0TopaBZprd"
      },
      "source": [
        "def ReportAccuracies(avg_acc, avg_recall,avg_f1):\n",
        "  ic_acc = st.t.interval(0.9, len(avg_acc) - 1, loc=np.mean(avg_acc), scale=st.sem(avg_acc))\n",
        "  ic_recall = st.t.interval(0.9, len(avg_recall) - 1, loc=np.mean(avg_recall), scale=st.sem(avg_recall))\n",
        "  ic_f1 = st.t.interval(0.9, len(avg_f1) - 1, loc=np.mean(avg_f1), scale=st.sem(avg_f1))\n",
        "  print('Mean Accuracy[{:.4f}] G-mean_acc[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_acc), gmean(avg_acc), ic_acc[0], ic_acc[1]))\n",
        "  print('Mean Recall[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_recall), ic_recall[0], ic_recall[1]))\n",
        "  print('Mean F1[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_f1), ic_f1[0], ic_f1[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDGCA4dbbMd0"
      },
      "source": [
        "def build_model(row,col,num_classes):\n",
        "\n",
        "  tf.keras.initializers.GlorotNormal(234)\n",
        "  input_layer = tf.keras.Input(shape=(row,col,1,))\n",
        "  \n",
        "  layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())(input_layer)\n",
        "  layer = tf.keras.layers.LSTM(28,return_sequences=True)(layer)\n",
        "  layer = tf.keras.layers.Dropout(0.2)(layer)\n",
        "  layer = tf.keras.layers.LSTM(28,return_sequences=True)(layer)\n",
        "  layer = tf.keras.layers.Dropout(0.2)(layer)\n",
        "  layerconv1 = tf.keras.layers.Conv1D(filters=30,kernel_size=5,  strides=1) (layer)\n",
        "  layerconv1 = tf.keras.layers.Activation('relu')(layerconv1)\n",
        "  layerconv2 = tf.keras.layers.Conv1D(filters=40,kernel_size= 10,strides=1) (layer)\n",
        "  layerconv2 = tf.keras.layers.Activation('relu')(layerconv2)\n",
        "  layerconv3 = tf.keras.layers.Conv1D(filters=50,kernel_size= 15,strides=1) (layer)\n",
        "  layerconv3 = tf.keras.layers.Activation('relu')(layerconv3)\n",
        "  layerconv4 = tf.keras.layers.Conv1D(filters=60,kernel_size= 20,strides=1) (layer)\n",
        "  layerconv4 = tf.keras.layers.Activation('relu')(layerconv4)\n",
        "  max1= tf.reduce_max(layerconv1, 1)\n",
        "  max2= tf.reduce_max(layerconv2, 1)\n",
        "  max3= tf.reduce_max(layerconv3, 1)\n",
        "  max4= tf.reduce_max(layerconv4, 1)\n",
        "  concat_layer = tf.keras.layers.concatenate([max1,max2,max3,max4],1)\n",
        "  layer= tf.keras.layers.Dense(num_classes, activation = 'softmax')   (concat_layer) \n",
        "\n",
        "    \n",
        " \n",
        "  \n",
        "    \n",
        "  model = tf.keras.Model(inputs=input_layer, outputs=layer)\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEZ2wmpIbUjb"
      },
      "source": [
        "batch_size = 128\n",
        "epochs = 100\n",
        "\n",
        "learning_rate = 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9rkRFdAbbJ7"
      },
      "source": [
        "def Train(X,y,folds):\n",
        "  avg_acc = []\n",
        "  avg_recall = []\n",
        "  avg_f1 = []\n",
        "  n_class = y.shape[1]\n",
        "  _, img_rows, img_cols = X.shape\n",
        "  X=X.reshape(X.shape[0],img_rows,img_cols,1)\n",
        "  _,img_rows, img_cols,_ = X.shape\n",
        "  for i in range(0, len(folds)):\n",
        "    train_idx = folds[i][0]\n",
        "    test_idx = folds[i][1]\n",
        "    X_train = X[train_idx]\n",
        "    X_test = X[test_idx]\n",
        "    \n",
        "    model=build_model(img_rows, img_cols, n_class)\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
        "   \n",
        "\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer,  metrics=['accuracy'])\n",
        "   \n",
        "    \n",
        "    model.fit(X_train, y[train_idx], batch_size=batch_size,epochs=epochs,verbose=2)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "    y_true = np.argmax(y[test_idx], axis=1)\n",
        "    acc_fold = accuracy_score(y_true, y_pred)\n",
        "    avg_acc.append(acc_fold)\n",
        "    recall_fold = recall_score(y_true, y_pred, average='macro')\n",
        "    avg_recall.append(recall_fold)\n",
        "    f1_fold = f1_score(y_true, y_pred, average='macro')\n",
        "    avg_f1.append(f1_fold)\n",
        "    print('Accuracy[{:.4f}] Recall[{:.4f}] F1[{:.4f}] at fold[{}]'.format(acc_fold, recall_fold, f1_fold ,i+1))\n",
        "    print('________________________________________________________________')\n",
        "    \n",
        "  return avg_acc, avg_recall,avg_f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu6pWPj4uWnI"
      },
      "source": [
        "tf.keras.backend.set_image_data_format('channels_last')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}